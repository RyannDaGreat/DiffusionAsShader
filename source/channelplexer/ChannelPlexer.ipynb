{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import torch.nn as nn\n",
    "import rp\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from diffusers import CogVideoXPipeline\n",
    "import itertools\n",
    "\n",
    "\n",
    "if not 'pipe' in dir():\n",
    "    device = rp.select_torch_device(prefer_used=True)\n",
    "    dtype = torch.float16\n",
    "    pipe = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-5b\", dtype=dtype)\n",
    "    pipe.vae=pipe.vae.to(device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n",
    "def retrieve_latents(\n",
    "    encoder_output: torch.Tensor, generator = None, sample_mode: str = \"sample\"\n",
    "):\n",
    "    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n",
    "        return encoder_output.latent_dist.sample(generator)\n",
    "    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n",
    "        return encoder_output.latent_dist.mode()\n",
    "    elif hasattr(encoder_output, \"latents\"):\n",
    "        return encoder_output.latents\n",
    "    else:\n",
    "        raise AttributeError(\"Could not access latents of provided encoder_output\")\n",
    "\n",
    "def ryan_encode_video(\n",
    "    pipe,\n",
    "    video,\n",
    "    *,\n",
    "    latents_form=\"BTCHW\",\n",
    "    individual_frames=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Encodes a list of PIL images into a video latent representation using the provided VAE and VideoProcessor.\n",
    "\n",
    "    Args:\n",
    "        video (List[PIL.Image.Image]): List of PIL images representing the video frames.\n",
    "        batch_size (int, optional): Batch size for encoding. Defaults to 1.\n",
    "        num_frames (int, optional): Number of frames in the video. Defaults to 49.\n",
    "        height (int, optional): Height of each frame. Defaults to 60.\n",
    "        width (int, optional): Width of each frame. Defaults to 90.\n",
    "        generator (Optional[torch.Generator], optional): Torch generator for reproducibility. Defaults to None.\n",
    "        device (str, optional): Device to run encoding on. Defaults to \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Video latent representation.\n",
    "\n",
    "        RETURNS IN FORM: BTCHW  (the default)\n",
    "            OR other forms: TCHW, CHW\n",
    "    \"\"\"\n",
    "    # COGVIDEOX ASSUMPTIONS\n",
    "    height = rp.get_video_height(video)\n",
    "    width = rp.get_video_width(video)\n",
    "    num_frames = len(video)\n",
    "    assert num_frames <= 49, num_frames  # Warning! Not strict error though.\n",
    "\n",
    "    if individual_frames:\n",
    "        #Encode each frame individually. Used for original first-frame behaviour.\n",
    "        video_latents = [\n",
    "            ryan_encode_video(\n",
    "                pipe,\n",
    "                [frame],\n",
    "                latents_form=\"BTCHW\",\n",
    "                individual_frames=False,\n",
    "            )\n",
    "            for frame in video\n",
    "        ]\n",
    "        video_latents = torch.cat(video_latents, dim=1)\n",
    "\n",
    "    else:\n",
    "        video = rp.as_rgb_images(video)\n",
    "        video = rp.as_pil_images(video)\n",
    "        vae = pipe.vae\n",
    "        video_processor = pipe.video_processor\n",
    "\n",
    "        video_tensor = video_processor.pil_to_numpy(video)\n",
    "        video_tensor = video_processor.numpy_to_pt(video_tensor)\n",
    "        video_tensor = video_tensor.to(device=vae.device, dtype=vae.dtype)\n",
    "\n",
    "        # Resize and crop images to match model input size\n",
    "        processed_video = []\n",
    "        for frame in video_tensor:\n",
    "            frame = video_processor.preprocess(frame, height=height, width=width)\n",
    "            processed_video.append(frame)\n",
    "        processed_video = torch.concatenate(processed_video)  # [F, C, H, W]\n",
    "        processed_video = processed_video.unsqueeze(0)  # [B=1, F, C, H, W]\n",
    "\n",
    "        processed_video = rearrange(processed_video, \"B F C H W -> B C F H W\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            video_latents = vae.encode(processed_video)\n",
    "            \n",
    "        video_latents = retrieve_latents(video_latents)\n",
    "        video_latents = [video_latents]  # Encode the entire video at once\n",
    "\n",
    "        video_latents = (\n",
    "            torch.cat(video_latents, dim=0).to(vae.dtype).permute(0, 2, 1, 3, 4)\n",
    "        )  # [B, C, F, H, W]\n",
    "        # <---- THIS COMMENT IS WRONG!! DISCOVERED BY CLARA. IT'S ACTUALLY BTCHW\n",
    "\n",
    "        if not vae.config.invert_scale_latents:\n",
    "            video_latents = vae.config.scaling_factor * video_latents\n",
    "        else:\n",
    "            video_latents = 1 / vae.config.scaling_factor * video_latents\n",
    "\n",
    "    if latents_form == \"BTCHW\":\n",
    "        return video_latents\n",
    "    elif latents_form == \"TCHW\":\n",
    "        return rearrange(video_latents, \"1 T C H W -> T C H W\")\n",
    "    elif latents_form == \"CHW\":\n",
    "        assert num_frames==1, num_frames\n",
    "        return rearrange(video_latents, \"1 1 C H W -> C H W\")\n",
    "    else:\n",
    "        assert False, latents_form\n",
    "\n",
    "def ryan_decode_latents(\n",
    "    pipe,\n",
    "    latents,\n",
    "    *,\n",
    "    latents_form=None,\n",
    "):\n",
    "    \"\"\"Returns a video\"\"\"\n",
    "    if latents_form is None: latents_form = {5:'BTCHW', 4:'TCHW', 3:'CHW'}[latents.ndim]\n",
    "    if latents_form == \"BTCHW\":\n",
    "        pass\n",
    "    elif latents_form == \"TCHW\":\n",
    "        latents = rearrange(latents, \"T C H W -> 1 T C H W\") #BTCHW\n",
    "    elif latents_form == \"CHW\":\n",
    "        latents = rearrange(latents,   \"C H W -> 1 1 C H W\") #BTCHW\n",
    "    else:\n",
    "        assert False, latents_form\n",
    "\n",
    "    with torch.no_grad():\n",
    "        video = pipe.decode_latents(latents)\n",
    "    \n",
    "    video = rearrange(video, \"1 C T H W -> T C H W\")\n",
    "    video = video / 2 + 0.5\n",
    "    video = rp.as_numpy_images(video)\n",
    "    \n",
    "    return video\n",
    "\n",
    "def encode(video, *, individual_frames=False):\n",
    "    video = rp.as_numpy_images(video)\n",
    "    return ryan_encode_video(\n",
    "        pipe,\n",
    "        video,\n",
    "        individual_frames=individual_frames,\n",
    "        latents_form=\"TCHW\",\n",
    "    )\n",
    "\n",
    "def decode(latents):\n",
    "    return ryan_decode_latents(pipe, latents.to(device=device, dtype=dtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_color_video(VT, VH, VW, VC):\n",
    "    colors = np.random.rand(VT, 1, 1, VC)\n",
    "    output = np.tile(colors, (1, VH, VW, 1))\n",
    "\n",
    "    output = output / 2 + np.roll(output, 1, 0) / 2\n",
    "    # output = output / 2 + np.roll(output, 1, 0) / 2\n",
    "    # output = output / 2 + np.roll(output, 1, 0) / 2\n",
    "    output = rp.full_range(output)\n",
    "    \n",
    "    assert output.shape == (VT, VH, VW, VC)\n",
    "    return output\n",
    "    \n",
    "rp.display_image_slideshow(rp.as_numpy_images(random_color_video(49,128,128,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_urls = 'https://gist.githubusercontent.com/SqrtRyan/ef34afaa17a92503ea5f2928780b4497/raw/26e97b6a5d70b4464c1fd967cff9132a62cb1c12/gistfile1.txt'\n",
    "video_urls = rp.download_to_cache(video_urls)\n",
    "video_urls = rp.load_file_lines(video_urls, use_cache=True)\n",
    "video_urls = video_urls[:1000]\n",
    "\n",
    "video_paths = rp.download_files_to_cache(video_urls, show_progress=True)\n",
    "\n",
    "train_videos = rp.load_videos(video_paths, use_cache=True, show_progress=True, lazy=False, num_threads=100)\n",
    "\n",
    "def random_color_video(VT, VH, VW, VC):\n",
    "    assert VC==3\n",
    "\n",
    "    video = rp.random_element(train_videos)\n",
    "    video = rp.resize_list(video, VT)\n",
    "    video = rp.as_float_images(video)\n",
    "    video = rp.as_rgb_images(video)\n",
    "    video = rp.resize_images(video, size=(VH, VW))\n",
    "\n",
    "    output = video\n",
    "    \n",
    "    assert output.shape == (VT, VH, VW, VC)\n",
    "    return output\n",
    "    \n",
    "rp.display_image_slideshow(rp.as_numpy_images(random_color_video(49,128,128,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VH = VW = 128\n",
    "VH, VW = 60*8, 90*8\n",
    "\n",
    "VT, VH, VW, VC = 49, VH, VW, 3\n",
    "LT, LH, LW, LC = 13, VH//8, VW//8, 16\n",
    "\n",
    "TI = VT\n",
    "TO = LT\n",
    "CI = CO = LC\n",
    "\n",
    "TICI = TI * CI\n",
    "TOCO = TO * CO\n",
    "LHLW = LH * LW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(video=None):\n",
    "    if video is None: video = random_color_video(VT, VH, VW, VC)\n",
    "\n",
    "    #Make sure they're small and fast...\n",
    "    # video = rp.resize_images(video, size=(VH, VW)) \n",
    "    \n",
    "    latents = encode(video, individual_frames=False)\n",
    "\n",
    "    ilatents = encode(video, individual_frames=True) #Individual Latents\n",
    "\n",
    "    latents = latents.to('cpu')\n",
    "    ilatents = ilatents.to('cpu')\n",
    "    \n",
    "    return rp.gather_vars('video latents ilatents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rp.globalize_locals\n",
    "def demo_get_sample():\n",
    "    sample = get_sample()\n",
    "    video, latents, ilatents = rp.destructure(sample)\n",
    "    pred_video = decode(latents)\n",
    "    preview_video = rp.horizontally_concatenated_videos(rp.as_numpy_videos(rp.resize_lists_to_max_len([video, pred_video])))\n",
    "    rp.display_image_slideshow(preview_video)\n",
    "    rp.display_image(rp.horizontally_concatenated_images(rp.rotate_images(preview_video,angle=90)))\n",
    "demo_get_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_folder = f'/pcached_samples_videos_{VH}_{VW}'\n",
    "rp.fansi_print(sample_folder, 'bold white blue cyan')\n",
    "num_samples = 15\n",
    "sample_paths = rp.path_join(sample_folder, list(map(str,range(num_samples))))\n",
    "samples = [rp.file_cache_call(path, get_sample) for path in rp.eta(sample_paths, 'Making Samples')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(LI, W):\n",
    "    # assert LI.shape == (TI, CI, LH, LW)\n",
    "    # assert W.shape == (TICI, TOCO)\n",
    "    LI = rearrange(LI, 'TI CI LH LW -> (LH LW) (TI CI)')\n",
    "    # assert LI.shape == (LHLW, TICI)\n",
    "    LO = LI @ W\n",
    "    # assert LO.shape == (LHLW, TOCO)\n",
    "    LO = rearrange(LO, '(LH LW) (TO CO) -> TO CO LH LW', TO=TO, CO=CO, LH=LH, LW=LW)\n",
    "    # assert LO.shape == (TO, CO, LH, LW)\n",
    "    return LO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_mask = torch.eye(LC, device=device, dtype=dtype).repeat((TI,TO))\n",
    "W_mask[:]=1 #DISABLE THE MASK\n",
    "rp.display_image(rp.as_rgb_image(rp.as_numpy_array(W_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(TICI, TOCO, dtype=dtype, device=device, requires_grad=True)\n",
    "# W = nn.Parameter(W)\n",
    "optim = torch.optim.SGD(params=[W], lr=.01, momentum=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iters = 10000\n",
    "losses = []\n",
    "batch_size = 10\n",
    "for train_iter in range(1, train_iters+1):\n",
    "    loss = 0\n",
    "    for _ in range(batch_size):\n",
    "        # W = W * W_mask\n",
    "        \n",
    "        sample = rp.random_element(samples)\n",
    "        L  = sample.latents.to(device) + 0\n",
    "        LI = sample.ilatents.to(device) + 0\n",
    "        LO = f(LI, W * W_mask)\n",
    "        assert L.shape == LO.shape\n",
    "        \n",
    "        loss = loss + F.mse_loss(L, LO)\n",
    "\n",
    "    losses.append(float(loss))\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if rp.toc() > 1 or train_iter==train_iters:\n",
    "        print(f'{train_iter: >7}    {float(loss):.03}')\n",
    "        rp.tic()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rp.globalize_locals\n",
    "def demo_get_sample():\n",
    "    sample = get_sample()\n",
    "    video, latents, ilatents = rp.destructure(sample)\n",
    "    latents = latents.to(device)\n",
    "    ilatents = ilatents.to(device)\n",
    "    pred_video = decode(latents)\n",
    "    f_pred_video = decode(f(ilatents, W))\n",
    "    preview_video = rp.horizontally_concatenated_videos(rp.as_numpy_videos(rp.resize_lists_to_max_len([video, pred_video, f_pred_video])))\n",
    "    rp.display_image_slideshow(preview_video)\n",
    "    rp.display_image(rp.horizontally_concatenated_images(rp.rotate_images(preview_video,angle=90)))\n",
    "demo_get_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rp.globalize_locals\n",
    "def load_in_video(url):\n",
    "    in_video = url\n",
    "    # in_video = 'https://videos.pexels.com/video-files/2795691/2795691-uhd_2560_1440_25fps.mp4'\n",
    "    # in_video = 'https://videos.pexels.com/video-files/6507082/6507082-hd_1920_1080_25fps.mp4'\n",
    "    # in_video = 'https://videos.pexels.com/video-files/6507468/6507468-hd_1920_1080_25fps.mp4'\n",
    "    in_video = rp.download_to_cache(in_video)\n",
    "    in_video = rp.load_video(in_video, use_cache=True)\n",
    "    in_video = rp.resize_list(in_video, VT)\n",
    "    in_video = rp.resize_images(in_video, size=(VH, VW))\n",
    "    rp.display_video(in_video, framerate=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with rp.TemporarilySetItem(globals(), dict(VH = 64, VW = 64, LW=8, LH=8)):\n",
    "@rp.globalize_locals\n",
    "def demo_get_sample():\n",
    "    global video\n",
    "    sample = get_sample(in_video[::-1])\n",
    "    video, latents, ilatents = rp.destructure(sample)\n",
    "    latents = latents.to(device)\n",
    "    ilatents = ilatents.to(device)\n",
    "    pred_video = decode(latents)\n",
    "    f_pred_video = decode(f(ilatents, W))\n",
    "    preview_video = rp.vertically_concatenated_videos(rp.as_numpy_videos(rp.resize_lists_to_max_len([video, pred_video, f_pred_video])))\n",
    "    preview_video = rp.labeled_images(preview_video, range(1,len(preview_video)+1), font='R:Futura')\n",
    "    # rp.display_image_slideshow(preview_video)\n",
    "    # rp.display_image(rp.horizontally_concatenated_images(preview_video))\n",
    "    rp.display_video(rp.resize_images(preview_video,size=.25))\n",
    "\n",
    "for url in [\n",
    "    'https://videos.pexels.com/video-files/7515906/7515906-hd_1920_1080_30fps.mp4'\n",
    "    # 'https://videos.pexels.com/video-files/3127017/3127017-uhd_2560_1440_24fps.mp4',\n",
    "    # 'https://videos.pexels.com/video-files/1851190/1851190-uhd_2560_1440_25fps.mp4',\n",
    "    # 'https://videos.pexels.com/video-files/857195/857195-hd_1280_720_25fps.mp4',\n",
    "    # 'https://videos.pexels.com/video-files/4061791/4061791-hd_1920_1080_24fps.mp4',\n",
    "    # 'https://videos.pexels.com/video-files/7170786/7170786-uhd_2732_1440_25fps.mp4',\n",
    "]:\n",
    "    load_in_video(url)\n",
    "    demo_get_sample()\n",
    "    rp.save_video_mp4(preview_video, rp.get_unique_copy_path('linear_combinations_interp.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.display_video(preview_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.save_video_mp4(preview_video, rp.get_unique_copy_path('linear_combinations_interp.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.display_image(np.abs(rp.as_rgb_image(rp.as_numpy_array(W))))\n",
    "rp.display_image(rp.full_range(np.log(.01+np.abs(rp.as_rgb_image(rp.as_numpy_array(W))))))\n",
    "# rp.display_image(rp.as_rgb_image(rp.as_numpy_array(rearrange(W, '(TI CI) (TO CO) -> (TI TO) (CI CO)', TO=TO, CO=CO, TI=TI, CI=CI ))))\n",
    "# rp.display_image(rp.as_rgb_image(rp.as_numpy_array(rearrange(W, '(TI CI) (TO CO) -> (TO TI) (CO CI)', TO=TO, CO=CO, TI=TI, CI=CI ))))\n",
    "\n",
    "rp.display_image(rp.full_range(rp.cv_resize_image(rp.cv_resize_image(np.abs(rp.as_rgb_image(rp.as_numpy_array(W))), 1/16),16,interp='nearest')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.object_to_file(W,'W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.display_video(\n",
    "    rp.horizontally_concatenated_videos(rp.resize_lists_to_max_len(video,\n",
    "    rp.resize_images(\n",
    "        rp.full_range(\n",
    "            rp.as_numpy_images(\n",
    "                latents[:, :3],\n",
    "            ),\n",
    "        ),\n",
    "        size=(480, 720),\n",
    "        interp=\"nearest\",\n",
    "    ))),framerate=20,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python cogvid",
   "language": "python",
   "name": "cogvid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
