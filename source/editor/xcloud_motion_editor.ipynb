{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## JUPYTER MULTIPLIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################\n",
    "import rp\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "# --- Global state to track the currently registered handler ---\n",
    "# This is the key to preventing multiple hooks from being registered.\n",
    "_CURRENT_JUPYTER_HOOK_HANDLER = None\n",
    "\n",
    "def set_jupyter_code_hook(handler):\n",
    "    \"\"\"\n",
    "    Registers a function to run before a Python code cell is executed.\n",
    "\n",
    "    This function ensures that any previously registered hook is automatically\n",
    "    unregistered. It creates a wrapper so the provided handler is only\n",
    "    called for Python code and receives only the code string.\n",
    "\n",
    "    Args:\n",
    "        handler (function): A function to be called before cell execution.\n",
    "                            It must accept one argument: the code string.\n",
    "    \"\"\"\n",
    "    global _CURRENT_JUPYTER_HOOK_HANDLER\n",
    "    ipython_shell = get_ipython()\n",
    "\n",
    "    # If a handler was previously registered by this framework, unregister it first.\n",
    "    if _CURRENT_JUPYTER_HOOK_HANDLER:\n",
    "        try:\n",
    "            ipython_shell.events.unregister('pre_run_cell', _CURRENT_JUPYTER_HOOK_HANDLER)\n",
    "        except ValueError:\n",
    "            # This can happen if the hook was manually removed. It's safe to ignore.\n",
    "            pass\n",
    "\n",
    "    def _wrapper_handler(info):\n",
    "        \"\"\"The actual function registered with IPython's event system.\"\"\"\n",
    "        code_string = info.raw_cell.strip()\n",
    "\n",
    "        # --- Filter for Python code ---\n",
    "        # Ignore empty cells, shell commands (like !pip), and cell magics (like %%time).\n",
    "        if code_string and rp.is_valid_python_syntax(code_string):\n",
    "            # If it's Python code, call the user's handler with the code string.\n",
    "            rp.fansi_print(\"--- RUNNING CODE ON ALL THE PARROTS ---\",'yellow bold')\n",
    "            handler(code_string)\n",
    "        \n",
    "        global WORKER\n",
    "        if code_string.startswith('#WORKER') and not WORKER:\n",
    "            #IF we're not a WORKER, don't run cells labled #WORKER at the top\n",
    "            rp.fansi_print(\"--- SKIPPING CELL AS I AM NOT A WORKER ---\",'yellow bold')\n",
    "            # Call the user's handler. It will decide if we should stop.\n",
    "            # The handler should return True to stop execution.\n",
    "            if handler(code_string) is True:\n",
    "                # THIS IS THE CORRECT WAY: replace the upcoming execution with an empty string.\n",
    "                ipython_shell.set_next_input(\"\", replace=True)\n",
    "\n",
    "    # Register the new wrapper handler and store a reference to it.\n",
    "    ipython_shell.events.register('pre_run_cell', _wrapper_handler)\n",
    "    _CURRENT_JUPYTER_HOOK_HANDLER = _wrapper_handler\n",
    "    print(f\"ðŸ¦† Hook Activated: Handler '{handler.__name__}' is now active.\")\n",
    "\n",
    "\n",
    "\n",
    "def deactivate_jupyter_code_hook():\n",
    "    \"\"\"\n",
    "    Finds and unregisters the currently active cell execution hook.\n",
    "    \"\"\"\n",
    "    global _CURRENT_JUPYTER_HOOK_HANDLER\n",
    "    ipython_shell = get_ipython()\n",
    "\n",
    "    if _CURRENT_JUPYTER_HOOK_HANDLER:\n",
    "        try:\n",
    "            ipython_shell.events.unregister('pre_run_cell', _CURRENT_JUPYTER_HOOK_HANDLER)\n",
    "            print(f\"ðŸ”‡ Hook Deactivated: Handler was removed.\")\n",
    "            _CURRENT_JUPYTER_HOOK_HANDLER = None\n",
    "        except ValueError:\n",
    "            print(\"Could not find the registered hook. It may have already been removed.\")\n",
    "    else:\n",
    "        print(\"No active hook was found to deactivate.\")\n",
    "\n",
    "#############################\n",
    "\n",
    "LOCAL_ONLY = False\n",
    "LOCAL_ONLY = True\n",
    "\n",
    "#DELEGATE ALL CODE FROM THIS NOTEBOOK TO MANY WORKERS SO THEY DUPLICATE IT ALL\n",
    "\n",
    "#Do we delegate tasks? If so we're the master\n",
    "DELEGATOR = rp.running_in_ipython()\n",
    "WORKER = not DELEGATOR\n",
    "\n",
    "if DELEGATOR:\n",
    "    import rp.web_evaluator\n",
    "    \n",
    "    cluster_info = rp.web_evaluator.launch_tmux_delegation_cluster(8,session_name='JupyterParrots',if_exists='replace')\n",
    "    \n",
    "    def do_all(code, **vars):\n",
    "        return cluster_info.delegator.evaluate_all(code + \"\\npass;\", **vars)\n",
    "\n",
    "    if not LOCAL_ONLY:\n",
    "        set_jupyter_code_hook(do_all)\n",
    "    else:\n",
    "        WORKER=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## TRACKING HELPERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX GLITCHY SCROLLING\n",
    "# https://github.com/jupyterlab/jupyterlab/issues/15968\n",
    "from IPython.display import HTML,display\n",
    "display(HTML(\"<style>.jp-WindowedPanel-viewport { contain: layout }</style>\"))\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK']='1' #For if using cotracker + mac + MPS\n",
    "\n",
    "import rp\n",
    "import einops\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "from icecream import ic\n",
    "import functools\n",
    "\n",
    "das_root = rp.printed(rp.get_absolute_path('../..'))\n",
    "sys.path.append(das_root)\n",
    "sys.path.append(rp.printed(rp.get_absolute_path(f'{das_root}/source/gaussblobs')))\n",
    "from source.gaussblobs.render_tracks import draw_blobs_videos, col26 as ordered_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKER\n",
    "if rp.currently_running_mac(): \n",
    "    device = torch.device('cpu') #Don't use MPS - we're using TapNext beacuse it's a lot better\n",
    "else:\n",
    "    device = rp.select_torch_device(reserve=True, prefer_used=True)\n",
    "    \n",
    "ic(das_root, device);\n",
    "\n",
    "# TAPNEXT SETUP FOR XCLOUD\n",
    "# ON XCLOUD, IF I NEED TO MODIFY IT LATER, USE THIS INSTEAD:\n",
    "sys.path += rp.get_absolute_paths(\n",
    "    \"/home/jupyter/CleanCode/Github/tapnet.git\",\n",
    ")\n",
    "from run_tapnet import run_tapnet\n",
    "tapnet_model_dir = \"/home/jupyter/CleanCode/Github/tapnet.git/model\"\n",
    "run_tapnet = functools.partial(run_tapnet, model_dir=tapnet_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "T,H,W=49,480,720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rp.globalize_locals\n",
    "def init_input_video():\n",
    "    # /Users/burgert/Downloads/MotionEditIPYBundle/MakeItNotBounce.mp4\n",
    "    input_video = rp.load_video(input_video_path, use_cache=True)\n",
    "    input_video = rp.resize_list(input_video, 49)\n",
    "    input_video = rp.resize_images_to_hold(input_video, height=480, width=720)\n",
    "    input_video = rp.crop_images(input_video, height=480, width=720, origin='center')\n",
    "    input_video = rp.as_float_images(input_video)\n",
    "    input_video = rp.as_numpy_array(input_video)\n",
    "    \n",
    "    ic(input_video_path, prompt, TITLE)\n",
    "    rp.display_video(gridded_video(input_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridded_video(input_video):\n",
    "    grid_input_video = rp.as_float_images(input_video)\n",
    "\n",
    "    alpha = .5\n",
    "    \n",
    "    grid_input_video[:,::20,:] =rp.blend(.5, grid_input_video[:,::20,:] , alpha)\n",
    "    grid_input_video[:,:,::20] =rp.blend(.5, grid_input_video[:,:,::20] , alpha)\n",
    "    \n",
    "    grid_input_video[:,::100,:]=rp.blend(1 , grid_input_video[:,::100,:], alpha)\n",
    "    grid_input_video[:,:,::100]=rp.blend(1 , grid_input_video[:,:,::100], alpha)\n",
    "    \n",
    "    grid_input_video = rp.labeled_images(grid_input_video, range(len(grid_input_video)), size=30, font='Arial')\n",
    "    grid_input_video = rp.video_with_progress_bar(grid_input_video, bar_color='green', position='top', size=5)\n",
    "\n",
    "    return grid_input_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points_on_video(video, points, colors=None, visible=None):\n",
    "    if colors is None:\n",
    "        colors=globals()['colors']\n",
    "    if visible is None:\n",
    "        visible = [True] * len(points)\n",
    "        \n",
    "    output = list(video)\n",
    "    for (t, x, y), color, v in zip(points, colors, visible):\n",
    "        if v:\n",
    "            output[t] = rp.cv_draw_circle(output[t], x, y, radius=15, rim=3, color=color, copy=False)\n",
    "            \n",
    "    output = rp.as_byte_images(output, copy=False)\n",
    "    return np.stack(output)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "# Assuming 'rp' is a pre-existing library with drawing utilities \n",
    "# and 'colors' is a globally defined list.\n",
    "\n",
    "def draw_points_on_video(video, points, colors=None, visible=None):\n",
    "    \"\"\"\n",
    "    Draws circles and their indices on a video sequence using OpenCV.\n",
    "    The text color is chosen to contrast with the circle color.\n",
    "    Handles both integer (0-255) and float (0-1) color formats.\n",
    "\n",
    "    Args:\n",
    "        video: The source video frames.\n",
    "        points: A list of points, where each point is a tuple (t, x, y)\n",
    "                representing the frame index, and x, y coordinates.\n",
    "        colors (list, optional): A list of colors for each point. \n",
    "                                 Defaults to a global 'colors' variable.\n",
    "        visible (list, optional): A list of booleans indicating if a point \n",
    "                                  is visible. Defaults to all True.\n",
    "    \"\"\"\n",
    "    if colors is None:\n",
    "        # Fallback to a global 'colors' variable if not provided\n",
    "        colors = globals()['colors']\n",
    "    if visible is None:\n",
    "        # Default to all points being visible\n",
    "        visible = [True] * len(points)\n",
    "        \n",
    "    output = list(video)\n",
    "    \n",
    "    # Use enumerate to get the index 'i' for each point\n",
    "    for i, ((t, x, y), color, v) in enumerate(zip(points, colors, visible)):\n",
    "        if v:\n",
    "            # First, draw the circle for the point as in the original function\n",
    "            output[t] = rp.cv_draw_circle(output[t], x, y, radius=15, rim=3, color=color, copy=False)\n",
    "            \n",
    "            # --- Updated functionality with OpenCV ---\n",
    "            # Prepare the text (the index of the point)\n",
    "            text = str(i)\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.7\n",
    "            font_thickness = 2\n",
    "            \n",
    "            # --- Added functionality: Choose contrasting text color ---\n",
    "            # Create a temporary color variable for luminance calculation\n",
    "            # to handle both float (0-1) and int (0-255) color formats.\n",
    "            calc_color = color\n",
    "            # Heuristic to check if color is in float format (e.g., values <= 1.0)\n",
    "            if max(color) <= 1.0:\n",
    "                # Convert to 0-255 scale for luminance calculation\n",
    "                calc_color = [int(c * 255) for c in color]\n",
    "\n",
    "            # Assuming color is in BGR format (standard for OpenCV)\n",
    "            # Calculate luminance to determine if the color is light or dark.\n",
    "            # For BGR: Y = 0.114*B + 0.587*G + 0.299*R\n",
    "            luminance = 0.114 * calc_color[0] + 0.587 * calc_color[1] + 0.299 * calc_color[2]\n",
    "            \n",
    "            # Use black text for light backgrounds, white for dark backgrounds\n",
    "            text_color = (0, 0, 0) if luminance > 140 else (255, 255, 255)\n",
    "            # --- End of added functionality ---\n",
    "\n",
    "            # Get the size of the text box to center it accurately\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "            \n",
    "            # Calculate the bottom-left corner of the text to center it inside the circle\n",
    "            text_x = x - text_width // 2\n",
    "            text_y = y + text_height // 2\n",
    "            \n",
    "            # Draw the index number using OpenCV's putText function.\n",
    "            # The text is drawn in white for good contrast against various circle colors.\n",
    "            # Note: cv2.putText modifies the image in place.\n",
    "            # The 'org' parameter (the coordinates) MUST be a tuple of integers.\n",
    "            cv2.putText(\n",
    "                output[t], \n",
    "                text, \n",
    "                (int(text_x), int(text_y)), \n",
    "                font, \n",
    "                font_scale, \n",
    "                text_color, # Use the dynamically chosen color\n",
    "                font_thickness,\n",
    "                lineType=cv2.LINE_AA # Gives smoother text\n",
    "            )\n",
    "            # --- End of updated functionality ---\n",
    "            \n",
    "    # Convert the list of frames back to a NumPy array of byte images\n",
    "    output = rp.as_byte_images(output, copy=False)\n",
    "    return np.stack(output)\n",
    "\n",
    "@rp.globalize_locals\n",
    "def init_tracks():\n",
    "    global init_points\n",
    "    # colors = 'white red green blue cyan magenta yellow'.split()\n",
    "    colors = ordered_colors[:len(init_points)]\n",
    "    init_points = np.stack(init_points)\n",
    "    \n",
    "    cotracker_tracks, cotracker_visibles = rp.run_cotracker(\n",
    "        input_video, \n",
    "        device = device,\n",
    "        queries = init_points,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        tracks, visibles = run_tapnet(\n",
    "            input_video, \n",
    "            device = device,\n",
    "            queries = init_points,\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\"FAILED TO RUN TAPNET, FALLING BACK TO COTRACKER\")\n",
    "        tracks, visibles = cotracker_tracks, cotracker_visibles\n",
    "\n",
    "    \n",
    "    # tracks   = torch.tensor(tracks  ).to(device=device)\n",
    "    # visibles = torch.tensor(visibles).to(device=device)\n",
    "    def draw_tracks(input_video, tracks=tracks, visibles=visibles):\n",
    "        track_preview_video = input_video\n",
    "        \n",
    "        for t  in rp.eta(range(T), 'draw_tracks'):\n",
    "            track = tracks[t]\n",
    "            visible = visibles[t]\n",
    "        \n",
    "            points = [[t, x, y] for x,y in track]\n",
    "            \n",
    "            track_preview_video = draw_points_on_video(track_preview_video, points, colors, visible)\n",
    "    \n",
    "        return track_preview_video\n",
    "    # visibles[:]=1\n",
    "    \n",
    "    T, N = rp.validate_tensor_shapes(\n",
    "        tracks             = 'torch: T N XY',\n",
    "        cotracker_tracks   = 'torch: T N XY',\n",
    "        visibles           = 'torch: T N',\n",
    "        cotracker_visibles = 'torch: T N',\n",
    "        init_points = 'numpy:   N TXY',\n",
    "        input_video = 'numpy: T H W RGB',\n",
    "        TXY=3,\n",
    "        XY=2,\n",
    "        RGB=3,\n",
    "        return_dims='T N',\n",
    "    )\n",
    "    \n",
    "    track_preview_video = draw_tracks(input_video)\n",
    "    \n",
    "    rp.display_video(gridded_video(track_preview_video))\n",
    "\n",
    "    num_tracks = tracks.shape[1]\n",
    "    blob_colors = ordered_colors[:num_tracks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Track Modifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_drift(tracks, *i, dx=0, dy=0, t_origin=0, do_before=True, do_after=True):\n",
    "    \"\"\"\n",
    "    Adds a drift to tracks at selected indices i\n",
    "    The drift starts at t_origin and for every timestep before and after dx and dy are added to its x and y\n",
    "    \"\"\"\n",
    "    new_tracks = tracks + 0\n",
    "    \n",
    "    # Create time array\n",
    "    ts = torch.arange(T).to(tracks.device, tracks.dtype)\n",
    "    \n",
    "    # Calculate drift for each timestep relative to t_origin\n",
    "    drift_x = (ts - t_origin) * dx\n",
    "    drift_y = (ts - t_origin) * dy\n",
    "    \n",
    "    # Apply drift to selected track indices\n",
    "    for idx in i:\n",
    "        new_tracks[:, idx, 0] += drift_x\n",
    "        new_tracks[:, idx, 1] += drift_y\n",
    "\n",
    "    if not do_before:\n",
    "        new_tracks[:t_origin] = tracks[:t_origin]\n",
    "\n",
    "    if not do_after:\n",
    "        new_tracks[t_origin:] = tracks[t_origin:]\n",
    "    \n",
    "    return new_tracks\n",
    "\n",
    "\n",
    "def tween(tracks, i, txy0, txy1):\n",
    "    tracks = tracks + 0\n",
    "    \n",
    "    if isinstance(txy0, int): txy0 = [txy0, *tracks[txy0,i]]\n",
    "    if isinstance(txy1, int): txy1 = [txy1, *tracks[txy1,i]]\n",
    "    \n",
    "    t0, x0, y0 = txy0\n",
    "    t1, x1, y1 = txy1\n",
    "    for t in range(t0, t1+1):\n",
    "        a=rp.iblend(t, t0, t1)\n",
    "        x=rp.blend(x0, x1, a)\n",
    "        y=rp.blend(y0, y1, a)\n",
    "        tracks[t,i,0]=x\n",
    "        tracks[t,i,1]=y\n",
    "\n",
    "    return tracks\n",
    "\n",
    "def zoom_tracks(tracks, *i, t_origin=0, x_origin=None, y_origin=None, d_scale=1.03):\n",
    "    \"\"\"\n",
    "    Applies geometric scaling to tracks at selected indices i\n",
    "    The scaling starts at t_origin with scale factor changing by d_scale each frame\n",
    "    x_origin and y_origin default to the mean of the tracks at t_origin if None\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    new_tracks = tracks + 0\n",
    "    T = tracks.shape[0]\n",
    "    \n",
    "    # Set default origins to mean of tracks at t_origin if not specified\n",
    "    if x_origin is None:\n",
    "        x_origin = tracks[t_origin, :, 0].mean().item()\n",
    "    if y_origin is None:\n",
    "        y_origin = tracks[t_origin, :, 1].mean().item()\n",
    "    \n",
    "    # Create time array\n",
    "    ts = torch.arange(T).to(tracks.device, tracks.dtype)\n",
    "    \n",
    "    # Calculate scale factors for each timestep relative to t_origin\n",
    "    scale_factors = d_scale ** (ts - t_origin)\n",
    "    \n",
    "    # Apply scaling to selected track indices\n",
    "    for idx in i:\n",
    "        # Center coordinates around origin\n",
    "        centered_x = new_tracks[:, idx, 0] - x_origin\n",
    "        centered_y = new_tracks[:, idx, 1] - y_origin\n",
    "        \n",
    "        # Apply scaling\n",
    "        new_tracks[:, idx, 0] = centered_x * scale_factors + x_origin\n",
    "        new_tracks[:, idx, 1] = centered_y * scale_factors + y_origin\n",
    "    \n",
    "    return new_tracks\n",
    "\n",
    "\n",
    "\n",
    "def horz_mirror(tracks, *i, t_origin=0, x_origin=None):\n",
    "    \"\"\"\n",
    "    Horizontally mirrors tracks at selected indices i around x_origin\n",
    "    x_origin defaults to the mean x position of the tracks at t_origin if None\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    new_tracks = tracks + 0\n",
    "    \n",
    "    # Set default x_origin to mean of tracks at t_origin if not specified\n",
    "    if x_origin is None:\n",
    "        x_origin = tracks[t_origin, :, 0].mean().item()\n",
    "    \n",
    "    # Apply horizontal mirroring to selected track indices\n",
    "    for idx in i:\n",
    "        # Mirror x coordinates around x_origin\n",
    "        new_tracks[:, idx, 0] = 2 * x_origin - new_tracks[:, idx, 0]\n",
    "    \n",
    "    return new_tracks\n",
    "\n",
    "\n",
    "def horz_mirror_origins(tracks, *i, x_origin=None):\n",
    "    \"\"\"\n",
    "    Horizontally mirrors tracks at selected indices i by mirroring their centroid\n",
    "    For each frame: calculates mean position of selected points, mirrors that centroid, \n",
    "    then applies the delta to all selected points\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    new_tracks = tracks + 0\n",
    "    \n",
    "    if x_origin is None:\n",
    "        # Use center of all tracks as mirror axis\n",
    "        x_mirror = tracks[:, :, 0].mean(dim=1)  # Shape: [T]\n",
    "    else:\n",
    "        # Use custom x_origin as mirror axis\n",
    "        x_mirror = torch.full((tracks.shape[0],), x_origin, device=tracks.device, dtype=tracks.dtype)\n",
    "    \n",
    "    # Calculate centroid of selected points for each frame\n",
    "    selected_indices = torch.tensor(list(i), device=tracks.device)\n",
    "    centroid_x = tracks[:, selected_indices, 0].mean(dim=1)  # Shape: [T]\n",
    "    \n",
    "    # Mirror the centroid around the mirror axis\n",
    "    mirrored_centroid_x = 2 * x_mirror - centroid_x\n",
    "    \n",
    "    # Calculate delta (how much to move all points)\n",
    "    delta_x = mirrored_centroid_x - centroid_x\n",
    "    \n",
    "    # Apply delta to all selected points\n",
    "    for idx in i:\n",
    "        new_tracks[:, idx, 0] += delta_x\n",
    "    \n",
    "    return new_tracks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reverse_tracks(tracks, visibles, *indices):\n",
    "    tracks = tracks + 0\n",
    "    visibles = visibles + 0\n",
    "    for index in indices:\n",
    "        tracks[:,index]=tracks[:,index].flip(0)\n",
    "        visibles[:,index]=visibles[:,index].flip(0)\n",
    "    return tracks, visibles\n",
    "\n",
    "\n",
    "def speed_tracks(tracks, visibles, *i, factor=1.0, t_origin=0, do_before=True, do_after=True):\n",
    "    \"\"\"\n",
    "    Dilates tracks in time at selected indices i by the given factor\n",
    "    factor > 1.0 slows down motion (dilates time), factor < 1.0 speeds up motion\n",
    "    The dilation is centered at t_origin\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    new_tracks = tracks + 0\n",
    "    new_visibles = visibles + 0\n",
    "    T = tracks.shape[0]\n",
    "    \n",
    "    # Create time array\n",
    "    ts = torch.arange(T).to(tracks.device, tracks.dtype).float()\n",
    "    \n",
    "    # Calculate dilated time coordinates relative to t_origin\n",
    "    dilated_ts = (ts - t_origin) * factor + t_origin\n",
    "    \n",
    "    # Apply time dilation to selected track indices\n",
    "    for idx in i:\n",
    "        # Interpolate positions at dilated time points\n",
    "        \n",
    "        original_x = tracks[:, idx, 0]\n",
    "        original_y = tracks[:, idx, 1]\n",
    "        \n",
    "        # Use linear interpolation to get positions at dilated times\n",
    "        import torch.nn.functional as F\n",
    "        \n",
    "        # Reshape for interpolation: [1, 1, T] format\n",
    "        x_interp = F.interpolate(\n",
    "            original_x.unsqueeze(0).unsqueeze(0), \n",
    "            size=T, \n",
    "            mode='linear', \n",
    "            align_corners=True\n",
    "        ).squeeze()\n",
    "        \n",
    "        y_interp = F.interpolate(\n",
    "            original_y.unsqueeze(0).unsqueeze(0), \n",
    "            size=T, \n",
    "            mode='linear', \n",
    "            align_corners=True\n",
    "        ).squeeze()\n",
    "        \n",
    "        # Map dilated times to original indices for interpolation\n",
    "        valid_mask = (dilated_ts >= 0) & (dilated_ts < T-1)\n",
    "        dilated_ts_clamped = torch.clamp(dilated_ts, 0, T-1)\n",
    "        \n",
    "        # Linear interpolation manually\n",
    "        floor_indices = torch.floor(dilated_ts_clamped).long()\n",
    "        ceil_indices = torch.clamp(floor_indices + 1, 0, T-1)\n",
    "        alpha = dilated_ts_clamped - floor_indices.float()\n",
    "        \n",
    "        new_tracks[:, idx, 0] = (1 - alpha) * original_x[floor_indices] + alpha * original_x[ceil_indices]\n",
    "        new_tracks[:, idx, 1] = (1 - alpha) * original_y[floor_indices] + alpha * original_y[ceil_indices]\n",
    "        \n",
    "        # Apply same time dilation to visibles\n",
    "        original_vis = visibles[:, idx]\n",
    "        new_visibles[:, idx] = (1 - alpha) * original_vis[floor_indices].float() + alpha * original_vis[ceil_indices].float()\n",
    "        new_visibles[:, idx] = (new_visibles[:, idx] > 0.5).to(visibles.dtype)\n",
    "    \n",
    "    if not do_before:\n",
    "        new_tracks[:t_origin] = tracks[:t_origin]\n",
    "        new_visibles[:t_origin] = visibles[:t_origin]\n",
    "    \n",
    "    if not do_after:\n",
    "        new_tracks[t_origin:] = tracks[t_origin:]\n",
    "        new_visibles[t_origin:] = visibles[t_origin:]\n",
    "    \n",
    "    return new_tracks, new_visibles\n",
    "\n",
    "\n",
    "    \n",
    "def resize_list_linterp(values, length: int):\n",
    "    \"\"\"Resize tensor along first dimension using linear interpolation.\n",
    "    \n",
    "    Args:\n",
    "        values: Input tensor of any dimensionality >=1\n",
    "        length: Target length for first dimension\n",
    "        \n",
    "    Returns:\n",
    "        Tensor with shape (length, *values.shape[1:])\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    if length == 0:\n",
    "        return torch.tensor([])\n",
    "    if length == 1:\n",
    "        return values[-1:]\n",
    "    if len(values) == 1:\n",
    "        return values.repeat([length] + [1] * (values.ndim - 1))\n",
    "    \n",
    "    # Flatten all dimensions except first, interpolate, then reshape back\n",
    "    original_shape = values.shape\n",
    "    values_flat = values.view(original_shape[0], -1)\n",
    "    \n",
    "    interpolated = torch.nn.functional.interpolate(\n",
    "        values_flat.t().unsqueeze(0).float(),\n",
    "        size=length,\n",
    "        mode='linear',\n",
    "        align_corners=True\n",
    "    ).squeeze(0).t().to(values.dtype)\n",
    "    \n",
    "    return interpolated.view(length, *original_shape[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_arrows(video, old_tracks, new_tracks, old_visibles, new_visibles):\n",
    "    out=[]\n",
    "    for frame, old_track, new_track, old_viz, new_viz in zip(video, old_tracks, new_tracks, old_visibles, new_visibles):\n",
    "        start_x, start_y = old_track.T\n",
    "        end_x, end_y = new_track.T\n",
    "        visible = [ov * nv for ov,nv in zip(old_viz, new_viz)]\n",
    "        \n",
    "        frame = rp.cv_draw_arrows(frame, start_x, start_y, end_x, end_y, color=blob_colors, tip_length=0, visible=visible)\n",
    "        out.append(frame)\n",
    "    return rp.as_numpy_array(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rp.globalize_locals\n",
    "def display_tracks_diff(_secondary_video=None):\n",
    "    _secondary_video = _secondary_video if _secondary_video is not None else input_video\n",
    "    before_preview, after_preview = rp.labeled_videos(\n",
    "        [\n",
    "            gridded_video(draw_tracks(input_video, tracks, visibles)),\n",
    "            # gridded_video(draw_tracks(input_video, new_tracks, new_visibles)),\n",
    "            gridded_video(draw_arrows(draw_tracks(_secondary_video, new_tracks, new_visibles), tracks, new_tracks, visibles, new_visibles))\n",
    "        ],\n",
    "        [\"Counterfactual Input\", \"Target\"],\n",
    "        size=30,\n",
    "        font=\"Futura\",\n",
    "        \n",
    "    )\n",
    "    \n",
    "    rp.display_video(\n",
    "        rp.labeled_images(\n",
    "        rp.horizontally_concatenated_videos(\n",
    "            before_preview,\n",
    "            [rp.cv_resize_image(rp.bordered_image_solid_color((rp.pil_text_to_image('\\n>',font='Menlo',size=200)),color='black',thickness=30,),1/3)],\n",
    "            after_preview,\n",
    "            origin='center',\n",
    "        ),\n",
    "            f'{input_video_path}\\n{prompt}',font='Futura',position='bottom',size=20,size_by_lines=True,text_color='light blue',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracks_to_xyzv(tracks, visibles):\n",
    "    #T N XY -> T N XYZV\n",
    "\n",
    "    #I currently don't care about depth\n",
    "    z = torch.ones_like(tracks[:,:,0])\n",
    "\n",
    "    tracks_xyzv, _ = einops.pack([tracks, z, visibles], 'T N *')\n",
    "\n",
    "    rp.validate_tensor_shapes(\n",
    "        tracks      = 'torch: T N XY',\n",
    "        visibles    = 'torch: T N',\n",
    "        z           = 'torch: T N',\n",
    "        tracks_xyzv = 'torch: T N XYZV',\n",
    "        XYZV=4,\n",
    "        XY=2,\n",
    "    )\n",
    "\n",
    "    return tracks_xyzv\n",
    "\n",
    "@rp.globalize_locals\n",
    "def init_blob_videos():\n",
    "    global visibles, new_visibles\n",
    "    \n",
    "    rp.display_image(rp.labeled_image(rp.tiled_images([rp.uniform_float_color_image(64,64,color) for color in blob_colors   ],border_thickness=0), \"Chosen Blob Colors\", size=30))\n",
    "    rp.display_image(rp.labeled_image(rp.tiled_images([rp.uniform_float_color_image(64,64,color) for color in ordered_colors],border_thickness=0), \"All Blob Colors\", size=30))\n",
    "                                                                \n",
    "    blobs_videos = draw_blobs_videos(\n",
    "        video         = rp.as_torch_video(input_video),\n",
    "        counter_video = rp.as_torch_video(input_video),\n",
    "        video_tracks         = tracks_to_xyzv(new_tracks, new_visibles),\n",
    "        counter_tracks = tracks_to_xyzv(tracks, visibles),\n",
    "        sigma = 10,\n",
    "        blob_colors = blob_colors,\n",
    "    )\n",
    "    \n",
    "    video_gaussians, counter_video_gaussians = rp.destructure(blobs_videos)\n",
    "    \n",
    "    #RGBA -> RGB\n",
    "    video_gaussians         = video_gaussians        [:,:3]\n",
    "    counter_video_gaussians = counter_video_gaussians[:,:3]\n",
    "    \n",
    "    rp.validate_tensor_shapes(\n",
    "        video_gaussians         = 'torch: T 3 H W',\n",
    "        counter_video_gaussians = 'torch: T 3 H W',\n",
    "        input_video             = 'numpy: T H W 3',\n",
    "    )\n",
    "    \n",
    "    #In range [0, 1]\n",
    "    assert 0<=counter_video_gaussians.min()<=counter_video_gaussians.max()<=1\n",
    "    assert 0<=video_gaussians        .min()<=video_gaussians        .max()<=1\n",
    "    \n",
    "    rp.display_video(rp.tiled_videos(rp.as_numpy_videos([video_gaussians, counter_video_gaussians]),border_color='white',border_thickness=1))\n",
    "\n",
    "\n",
    "\n",
    "def uncamera(points: torch.Tensor, ref_points: torch.Tensor, origin_frame: int = 0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Removes camera motion from points using a cascading transformation fit.\n",
    "\n",
    "    This function stabilizes points by calculating the transformation that maps\n",
    "    reference points from each frame to a specified origin frame. It uses the\n",
    "    most powerful transformation possible given the number of reference points:\n",
    "    - 4+ points: Homography (perspective)\n",
    "    - 3 points: Affine (translation, rotation, scaling, shear)\n",
    "    - 2 points: Euclidean (translation, rotation, uniform scaling)\n",
    "    - 1 point: Translation only\n",
    "\n",
    "    Args:\n",
    "        points (torch.Tensor): A tensor of shape [T, N, 2] containing the\n",
    "            target points to be stabilized.\n",
    "        ref_points (torch.Tensor): A tensor of shape [T, M, 2] containing\n",
    "            the reference points for stabilization. M must be 1 or greater.\n",
    "        origin_frame (int, optional): The index of the frame to use as the\n",
    "            stable reference. All other frames will be aligned to this one.\n",
    "            Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A new tensor of shape [T, N, 2] with the camera\n",
    "            motion removed from the input `points`.\n",
    "    \"\"\"\n",
    "    # --- Input Validation and Setup ---\n",
    "    if not isinstance(points, torch.Tensor) or not isinstance(ref_points, torch.Tensor):\n",
    "        raise TypeError(\"Inputs 'points' and 'ref_points' must be PyTorch tensors.\")\n",
    "    if points.dim() != 3 or ref_points.dim() != 3 or points.shape[2] != 2 or ref_points.shape[2] != 2:\n",
    "        raise ValueError(\"Inputs must be of shape [T, N, 2] or [T, M, 2].\")\n",
    "    if points.shape[0] != ref_points.shape[0]:\n",
    "        raise ValueError(\"Both 'points' and 'ref_points' must have the same number of frames (T).\")\n",
    "        \n",
    "    num_ref_points = ref_points.shape[1]\n",
    "    if num_ref_points < 1:\n",
    "        raise ValueError(\n",
    "            f\"At least 1 reference point is required to calculate a transformation, \"\n",
    "            f\"but got {num_ref_points} points.\"\n",
    "        )\n",
    "\n",
    "    num_frames, _, _ = points.shape\n",
    "    device, dtype = points.device, points.dtype\n",
    "\n",
    "    new_points = torch.empty_like(points)\n",
    "    ref_points_np = ref_points.cpu().numpy().astype(np.float32)\n",
    "    ref_points_dst = ref_points_np[origin_frame]\n",
    "\n",
    "    # --- Fallback transformations for failed calculations ---\n",
    "    last_H = np.identity(3, dtype=np.float32)\n",
    "    last_A = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], dtype=np.float32)\n",
    "    last_T = np.zeros(2, dtype=np.float32)\n",
    "\n",
    "    # --- Main Loop: Iterate Through Frames ---\n",
    "    for t in range(num_frames):\n",
    "        if t == origin_frame:\n",
    "            new_points[t] = points[t].clone()\n",
    "            continue\n",
    "\n",
    "        ref_points_src = ref_points_np[t]\n",
    "        points_t_np = points[t].cpu().numpy().astype(np.float32)\n",
    "        transformed_points_t_np = None\n",
    "\n",
    "        # --- Find and Apply Transformation based on number of points ---\n",
    "        if num_ref_points >= 4:\n",
    "            H, _ = cv2.findHomography(ref_points_src, ref_points_dst, 0)\n",
    "            if H is None: H = last_H\n",
    "            else: last_H = H\n",
    "            points_t_reshaped = points_t_np.reshape(1, -1, 2)\n",
    "            transformed_points_t_np = cv2.perspectiveTransform(points_t_reshaped, H)\n",
    "\n",
    "        elif num_ref_points == 3:\n",
    "            A = cv2.getAffineTransform(ref_points_src, ref_points_dst)\n",
    "            if A is None: A = last_A\n",
    "            else: last_A = A\n",
    "            points_t_reshaped = points_t_np.reshape(1, -1, 2)\n",
    "            transformed_points_t_np = cv2.transform(points_t_reshaped, A)\n",
    "\n",
    "        elif num_ref_points == 2:\n",
    "            # estimateAffinePartial2D computes an optimal Euclidean transform\n",
    "            A, _ = cv2.estimateAffinePartial2D(ref_points_src, ref_points_dst)\n",
    "            if A is None: A = last_A\n",
    "            else: last_A = A\n",
    "            points_t_reshaped = points_t_np.reshape(1, -1, 2)\n",
    "            transformed_points_t_np = cv2.transform(points_t_reshaped, A)\n",
    "\n",
    "        elif num_ref_points == 1:\n",
    "            # Simple translation based on the single point's displacement\n",
    "            translation = ref_points_dst[0] - ref_points_src[0]\n",
    "            if np.isnan(translation).any(): translation = last_T\n",
    "            else: last_T = translation\n",
    "            # Apply translation by simple addition\n",
    "            transformed_points_t_np = points_t_np + translation\n",
    "\n",
    "        # --- Store Result ---\n",
    "        if transformed_points_t_np is not None:\n",
    "            # Reshape is needed for all matrix-based transforms\n",
    "            transformed_points_t_reshaped = transformed_points_t_np.reshape(-1, 2)\n",
    "            new_points[t] = torch.from_numpy(transformed_points_t_reshaped).to(device, dtype)\n",
    "        else:\n",
    "            # Fallback if something unexpected happens\n",
    "            new_points[t] = points[t].clone()\n",
    "\n",
    "    return new_points\n",
    "\n",
    "def recamera(points: torch.Tensor, ref_points: torch.Tensor, origin_frame: int = 0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Re-applies camera motion to points, acting as the inverse of uncamera.\n",
    "\n",
    "    This function transforms points from the coordinate system of a stable\n",
    "    origin frame back into the coordinate system of each original frame. It\n",
    "    calculates the inverse of the transformation used in `uncamera` by swapping\n",
    "    the source and destination reference points.\n",
    "\n",
    "    Args:\n",
    "        points (torch.Tensor): A tensor of shape [T, N, 2] containing the\n",
    "            stabilized points (presumably from `uncamera`).\n",
    "        ref_points (torch.Tensor): A tensor of shape [T, M, 2] containing\n",
    "            the same reference points used for the original `uncamera` call.\n",
    "        origin_frame (int, optional): The index of the frame that was used as\n",
    "            the stable reference in the `uncamera` call. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A new tensor of shape [T, N, 2] with the original\n",
    "            camera motion re-introduced to the input `points`.\n",
    "    \"\"\"\n",
    "    # --- Input Validation and Setup ---\n",
    "    if not isinstance(points, torch.Tensor) or not isinstance(ref_points, torch.Tensor):\n",
    "        raise TypeError(\"Inputs 'points' and 'ref_points' must be PyTorch tensors.\")\n",
    "    if points.dim() != 3 or ref_points.dim() != 3 or points.shape[2] != 2 or ref_points.shape[2] != 2:\n",
    "        raise ValueError(\"Inputs must be of shape [T, N, 2] or [T, M, 2].\")\n",
    "    if points.shape[0] != ref_points.shape[0]:\n",
    "        raise ValueError(\"Both 'points' and 'ref_points' must have the same number of frames (T).\")\n",
    "\n",
    "    num_ref_points = ref_points.shape[1]\n",
    "    if num_ref_points < 1:\n",
    "        raise ValueError(\n",
    "            f\"At least 1 reference point is required to calculate a transformation, \"\n",
    "            f\"but got {num_ref_points} points.\"\n",
    "        )\n",
    "\n",
    "    num_frames, _, _ = points.shape\n",
    "    device, dtype = points.device, points.dtype\n",
    "\n",
    "    new_points = torch.empty_like(points)\n",
    "    ref_points_np = ref_points.cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # The source for the inverse transform is the origin frame\n",
    "    ref_points_src_inv = ref_points_np[origin_frame]\n",
    "\n",
    "    # --- Fallback transformations for failed calculations ---\n",
    "    last_H = np.identity(3, dtype=np.float32)\n",
    "    last_A = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], dtype=np.float32)\n",
    "    last_T = np.zeros(2, dtype=np.float32)\n",
    "\n",
    "    # --- Main Loop: Iterate Through Frames ---\n",
    "    for t in range(num_frames):\n",
    "        if t == origin_frame:\n",
    "            new_points[t] = points[t].clone()\n",
    "            continue\n",
    "\n",
    "        # The destination for the inverse transform is the current frame t\n",
    "        ref_points_dst_inv = ref_points_np[t]\n",
    "        points_t_np = points[t].cpu().numpy().astype(np.float32)\n",
    "        transformed_points_t_np = None\n",
    "\n",
    "        # --- Find and Apply Inverse Transformation ---\n",
    "        # We find the inverse by swapping the source and destination points\n",
    "        if num_ref_points >= 4:\n",
    "            H, _ = cv2.findHomography(ref_points_src_inv, ref_points_dst_inv, 0)\n",
    "            if H is None: H = last_H\n",
    "            else: last_H = H\n",
    "            points_t_reshaped = points_t_np.reshape(1, -1, 2)\n",
    "            transformed_points_t_np = cv2.perspectiveTransform(points_t_reshaped, H)\n",
    "\n",
    "        elif num_ref_points == 3:\n",
    "            A = cv2.getAffineTransform(ref_points_src_inv, ref_points_dst_inv)\n",
    "            if A is None: A = last_A\n",
    "            else: last_A = A\n",
    "            points_t_reshaped = points_t_np.reshape(1, -1, 2)\n",
    "            transformed_points_t_np = cv2.transform(points_t_reshaped, A)\n",
    "\n",
    "        elif num_ref_points == 2:\n",
    "            A, _ = cv2.estimateAffinePartial2D(ref_points_src_inv, ref_points_dst_inv)\n",
    "            if A is None: A = last_A\n",
    "            else: last_A = A\n",
    "            points_t_reshaped = points_t_np.reshape(1, -1, 2)\n",
    "            transformed_points_t_np = cv2.transform(points_t_reshaped, A)\n",
    "\n",
    "        elif num_ref_points == 1:\n",
    "            # Inverse translation is the displacement from origin to current frame\n",
    "            translation = ref_points_dst_inv[0] - ref_points_src_inv[0]\n",
    "            if np.isnan(translation).any(): translation = last_T\n",
    "            else: last_T = translation\n",
    "            transformed_points_t_np = points_t_np + translation\n",
    "\n",
    "        # --- Store Result ---\n",
    "        if transformed_points_t_np is not None:\n",
    "            transformed_points_t_reshaped = transformed_points_t_np.reshape(-1, 2)\n",
    "            new_points[t] = torch.from_numpy(transformed_points_t_reshaped).to(device, dtype)\n",
    "        else:\n",
    "            # Fallback if something unexpected happens\n",
    "            new_points[t] = points[t].clone()\n",
    "            \n",
    "    return new_points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## DIFFUSION HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rp.globalize_locals\n",
    "def init_sample():\n",
    "    result_title = rp.get_file_name(input_video_path, include_file_extension=False)\n",
    "    \n",
    "    result_folder = f'untracked/gaussblob_tests/{result_title}'\n",
    "    result_folder = rp.get_unique_copy_path(result_folder)\n",
    "    rp.make_directory(result_folder)\n",
    "    \n",
    "    ic(result_folder)\n",
    "    \n",
    "    sample = rp.as_easydict(\n",
    "        frames               = rp.as_torch_video(input_video) * 2 - 1, #This one doesn't matter\n",
    "        counter_video_frames = rp.as_torch_video(input_video) * 2 - 1, \n",
    "        tracking_frames         = video_gaussians             * 2 - 1,\n",
    "        counter_tracking_frames = counter_video_gaussians     * 2 - 1,\n",
    "        prompt = prompt,\n",
    "    )\n",
    "    \n",
    "    #SWAP\n",
    "    # sample.frames         , sample.counter_video_frames    = sample.counter_video_frames   , sample.frames         \n",
    "    # sample.tracking_frames, sample.counter_tracking_frames = sample.counter_tracking_frames, sample.tracking_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rp.globalize_locals\n",
    "def init_mp4_files():\n",
    "    rp.display_video(sample.frames                  / 2 + 0.5)\n",
    "    rp.display_video(sample.tracking_frames         / 2 + 0.5)\n",
    "    rp.display_video(sample.counter_tracking_frames / 2 + 0.5)\n",
    "    rp.display_video(sample.counter_video_frames    / 2 + 0.5)\n",
    "    \n",
    "    frames                  = rp.as_numpy_images(sample.frames                  / 2 + 0.5)\n",
    "    tracking_frames         = rp.as_numpy_images(sample.tracking_frames         / 2 + 0.5)\n",
    "    counter_tracking_frames = rp.as_numpy_images(sample.counter_tracking_frames / 2 + 0.5)\n",
    "    counter_video_frames    = rp.as_numpy_images(sample.counter_video_frames    / 2 + 0.5)\n",
    "    \n",
    "    with rp.SetCurrentDirectoryTemporarily(result_folder):\n",
    "        frames_path                  = rp.save_video_mp4(frames                 , \"frames.mp4\",                  framerate=20, video_bitrate=\"max\", show_progress=False)\n",
    "        tracking_frames_path         = rp.save_video_mp4(tracking_frames        , \"tracking_frames.mp4\",         framerate=20, video_bitrate=\"max\", show_progress=False)\n",
    "        counter_tracking_frames_path = rp.save_video_mp4(counter_tracking_frames, \"counter_tracking_frames.mp4\", framerate=20, video_bitrate=\"max\", show_progress=False)\n",
    "        counter_video_frames_path    = rp.save_video_mp4(counter_video_frames   , \"counter_video_frames.mp4\",    framerate=20, video_bitrate=\"max\", show_progress=False)\n",
    "    \n",
    "    prompt = sample.prompt\n",
    "    \n",
    "    ic(\n",
    "        prompt                      ,\n",
    "        frames_path                 ,\n",
    "        tracking_frames_path        ,\n",
    "        counter_tracking_frames_path,\n",
    "        counter_video_frames_path   ,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rp.globalize_locals\n",
    "def do_diffusion():\n",
    "    global pipe, device\n",
    "    \n",
    "    ##########################\n",
    "    # IMPORTS\n",
    "    ##########################\n",
    "    \n",
    "    import sys\n",
    "    import os\n",
    "    import shlex\n",
    "    \n",
    "    from functools import cached_property\n",
    "    \n",
    "    import models.cogvideox_tracking as cogtrack\n",
    "    import rp\n",
    "    import torch\n",
    "    from icecream import ic\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    sys.path += rp.get_absolute_paths(\n",
    "        [\n",
    "            \"~/CleanCode/Management\",\n",
    "            # \"~/CleanCode/Github/DiffusionAsShader\",\n",
    "            # \"~/CleanCode/Datasets/Vids/Raw_Feb28\",\n",
    "            # \"~/CleanCode/Github/CogvideX-Interpolation-Mar23:MotionPrompting\",\n",
    "            # \"~/CleanCode/Github/CogvideX-Interpolation-Feb13:Inpainting\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    import syncutil\n",
    "    \n",
    "    device = rp.select_torch_device(prefer_used=True, reserve=True)\n",
    "    \n",
    "    ##########################\n",
    "    # FUNCTIONS\n",
    "    ##########################\n",
    "    \n",
    "    CKPT_folder = rp.path_join(das_root,'diffusion_shader_model_CKPT')\n",
    "    CKPT_transformer_folder = rp.path_join(CKPT_folder, 'transformer')\n",
    "    \n",
    "    def update_to_latest_checkpoint():\n",
    "    \n",
    "        # if not rp.folder_exists(CKPT_folder):\n",
    "        rp.r._run_sys_command(f'rm -rf {CKPT_folder}')\n",
    "        rp.r._run_sys_command(\n",
    "            f'cp -al /home/jupyter/CleanCode/Github/DiffusionAsShader/diffusion_shader_model {CKPT_folder}'\n",
    "            # f'cp -al /home/jupyter/CleanCode/Huggingface/CogVideoX-5b {CKPT_folder}'\n",
    "        )\n",
    "        \n",
    "        latest_transformer_checkpoint = checkpoint_root\n",
    "        \n",
    "        rp.fansi_print(f'Using checkpoint: {latest_transformer_checkpoint}','bold green undercurl')\n",
    "    \n",
    "        rp.r._run_sys_command(\n",
    "            \"rm\",\n",
    "            \"-rf\",\n",
    "            CKPT_transformer_folder,\n",
    "        )\n",
    "        rp.make_hardlink(\n",
    "            rp.path_join(latest_transformer_checkpoint, \"transformer\"),\n",
    "            CKPT_transformer_folder,\n",
    "            recursive=True,\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def get_maps(video_path):\n",
    "        from diffusers.utils import export_to_video, load_image, load_video\n",
    "    \n",
    "        video_path=rp.get_absolute_path(video_path)\n",
    "    \n",
    "        maps = load_video(video_path)\n",
    "        # Convert list of PIL Images to tensor [T, C, H, W]\n",
    "        maps = torch.stack(\n",
    "            [\n",
    "                torch.from_numpy(np.array(frame)).permute(2, 0, 1).float() / 255.0\n",
    "                for frame in maps\n",
    "            ]\n",
    "        )\n",
    "        maps = maps.to(device=device, dtype=torch.bfloat16)\n",
    "    \n",
    "        print(f\"Encoding tracking maps from {video_path}\")\n",
    "        maps = maps.unsqueeze(0)  # [B, T, C, H, W]\n",
    "        maps = maps.permute(0, 2, 1, 3, 4)  # [B, C, T, H, W]\n",
    "        \n",
    "        maps = maps * 2 - 1 #Normalize from [0,1] to [-1, 1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            latent_dist = pipe.vae.encode(maps).latent_dist\n",
    "            maps = latent_dist.sample() * pipe.vae.config.scaling_factor\n",
    "            maps = maps.permute(0, 2, 1, 3, 4)  # [B, F, C, H, W]\n",
    "        \n",
    "        return maps\n",
    "    \n",
    "    def load_video_first_frame(video_path):\n",
    "        return image_form(next(rp.load_video_stream(rp.get_absolute_path(video_path))))\n",
    "    \n",
    "    def image_form(image):\n",
    "        image=rp.as_rgb_image(image)\n",
    "        return rp.as_pil_image(image)\n",
    "    \n",
    "    @rp.globalize_locals\n",
    "    def run_pipe(\n",
    "        prompt                    = prompt                      ,\n",
    "        video_path                = frames_path                 ,\n",
    "        tracking_map_path         = tracking_frames_path        ,\n",
    "        counter_tracking_map_path = counter_tracking_frames_path,\n",
    "        counter_video_map_path    = counter_video_frames_path   ,\n",
    "    ):\n",
    "        ic(\n",
    "            prompt,\n",
    "            video_path,\n",
    "            tracking_map_path,\n",
    "            counter_tracking_map_path,\n",
    "            counter_video_map_path,\n",
    "        )\n",
    "    \n",
    "        # prompt = ''\n",
    "        # fansi_print(\"LOOK MA NO PROMPT\",'blue')\n",
    "    \n",
    "        pipeline_args = {\n",
    "            \"prompt\"                 : prompt,\n",
    "            \"image\"                  : load_video_first_frame(video_path),\n",
    "            \"tracking_image\"         : load_video_first_frame(tracking_map_path),\n",
    "            \"counter_tracking_image\" : load_video_first_frame(counter_tracking_map_path),\n",
    "            \"counter_video_image\"    : load_video_first_frame(counter_video_map_path),\n",
    "            \"tracking_maps\"          : get_maps(tracking_map_path),\n",
    "            \"counter_tracking_maps\"  : get_maps(counter_tracking_map_path),\n",
    "            \"counter_video_maps\"     : get_maps(counter_video_map_path),\n",
    "            \"negative_prompt\"        : \"The video is not of a high quality, it has a low resolution. Watermark present in each frame. The background is solid. Strange body and strange trajectory. Distortion.\",\n",
    "            \"height\"              : 480,\n",
    "            \"width\"               : 720,\n",
    "            \"num_frames\"          : 49,\n",
    "            \"use_dynamic_cfg\"     : True,\n",
    "            \"guidance_scale\"      : guidance_scale, #3 if rp.random_chance() else 6,\n",
    "            \"num_inference_steps\" : num_inference_steps,\n",
    "        }\n",
    "    \n",
    "        rp.display_dict(rp.gather(pipeline_args,'prompt negative_prompt height width num_frames use_dynamic_cfg guidance_scale num_inference_steps'.split(), as_dict=True))\n",
    "    \n",
    "        pipeline_args |= dict(          \n",
    "            use_image_conditioning=True,\n",
    "            # latent_conditioning_dropout=[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "            latent_conditioning_dropout=latent_conditioning_dropout,#[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], #Weird, not as good actually...\n",
    "            # latent_conditioning_dropout=[1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1], #Sparse...25%\n",
    "    \n",
    "            # use_image_conditioning=False,\n",
    "            # latent_conditioning_dropout=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], #Weird, not as good actually...\n",
    "            # latent_conditioning_dropout=[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "            # latent_conditioning_dropout=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "            #latent_conditioning_dropout=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        )\n",
    "    \n",
    "        pipeline_args = rp.as_easydict(pipeline_args)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            results=pipe(**pipeline_args)\n",
    "        \n",
    "        video=results.frames[0]\n",
    "        video=rp.as_numpy_images(video)\n",
    "        video = rp.labeled_images(\n",
    "            video,\n",
    "            f\"PROMPT={repr(prompt[:50])}\\nCFG={pipeline_args.guidance_scale} DYN-CFG={pipeline_args.use_dynamic_cfg} STEPS={pipeline_args.num_inference_steps} {''.join(map(str,pipeline_args['latent_conditioning_dropout']))}\",\n",
    "            size=-25,\n",
    "            background_color=\"translucent dark blue\",\n",
    "            size_by_lines=False,\n",
    "        )\n",
    "    \n",
    "        video = rp.as_numpy_array(video)\n",
    "        \n",
    "        return video\n",
    "    \n",
    "    ##########################\n",
    "    # SETTINGS\n",
    "    ##########################\n",
    "    \n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DiffusionAsShader/ckpts/your_ckpt_path/CounterChans2500100000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-4500'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DiffusionAsShader/ckpts/your_ckpt_path/CounterChans_RandomSpeed_2500_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-1100'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DiffusionAsShader/ckpts/your_ckpt_path/CounterChans_RandomSpeed_2500_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-6000'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DiffusionAsShader/ckpts/your_ckpt_path/CounterChans_RandomSpeed_WithDropout_2500_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-3000'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DiffusionAsShader/ckpts/your_ckpt_path/CounterChans_RandomSpeed_WithDropout_2500_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-9200'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DiffusionAsShader/ckpts/your_ckpt_path/CounterChans_RandomSpeed_WithDropout_2500_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-14700'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DiffusionAsShader/ckpts/your_ckpt_path/CounterChans_RandomSpeed_WithDropout_2500_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-29000'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DiffusionAsShader/ckpts/your_ckpt_path/CounterChans_RandomSpeed_WithDropout_2500_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-29000'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DiffusionAsShader/ckpts/your_ckpt_path/CounterChans_RandomSpeed_WithDropout_2500_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-29000'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DiffusionAsShader/ckpts/your_ckpt_path/CounterChans_BetterAug_WithDropout_50kSamp_T2V_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-9000'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DiffusionAsShader/ckpts/your_ckpt_path/CounterChans_BetterAug_WithDropout_50kSamp_T2V_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-9000'\n",
    "    checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterChans_FIXED_DATASET_BetterAug_WithDropout_50kSamp_T2V_from_scratch_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-3500'\n",
    "    checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterChans_FIXED_DATASET_BetterAug_WithDropout_50kSamp_T2V_from_scratch_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-4500'\n",
    "    checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterChans_FIXED_DATASET_BetterAug_WithDropout_50kSamp_T2V_from_scratch_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-13500'\n",
    "    checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterChans_FIXED_DATASET_BetterAug_WithDropout_50kSamp_T2V_from_scratch_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-23000'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterBlobs_WithSingleframe_ManyColors_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-2500'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterBlobs_WithSingleframe_ManyColors_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-2500'\n",
    "    checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterBlobs_SingleFrameONLY_ManyColors_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-500'\n",
    "    checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/gauss_blobs_track2pointONLY10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-1500'\n",
    "    checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterBlobs_WithSingleframe_ManyColors_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-3500'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterChans_FIXED_DATASET_BetterAug_WithDropout_50kSamp_T2V_from_scratch_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-23000'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterChans_FIXED_DATASET_BetterAug_WithDropout_50kSamp_T2V_from_scratch_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-27000'\n",
    "    checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterBlobs_WithSingleframe_ManyColors_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-4000'\n",
    "    checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterChans_FIXED_DATASET_BetterAug_WithDropout_50kSamp_T2V_from_scratch_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-30000'\n",
    "    # checkpoint_root = '/home/jupyter/CleanCode/Github/DaS_Trees/gauss_blobs/ckpts/your_ckpt_path/CounterBlobs_WithSingleframe_ManyColors_10000000__optimizer_adamw__lr-schedule_cosine_with_restarts__learning-rate_1e-4/checkpoint-8000'#Overfit?\n",
    "    \n",
    "    \n",
    "    checkpoint_title = rp.get_folder_name(checkpoint_root)\n",
    "    \n",
    "    USE_T2V=True\n",
    "    # USE_T2V=False\n",
    "    \n",
    "    if USE_T2V:\n",
    "        os.environ['T2V_TRANSFORMER_CHECKPOINT'] = \"/home/jupyter/CleanCode/Huggingface/CogVideoX-5b/transformer\"\n",
    "    \n",
    "    NO_CONTROLNET=False\n",
    "    if NO_CONTROLNET:\n",
    "        os.environ['DISABLE_CONTROLNET'] = \"True\"\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    # SETUP\n",
    "    ##########################\n",
    "        \n",
    "    latest_transformer_checkpoint = syncutil.sync_checkpoint_folder(checkpoint_root)\n",
    "    \n",
    "    # rp.set_current_directory('/home/jupyter/CleanCode/Github/DiffusionAsShader')\n",
    "    # if not rp.file_exists('source/datasets/youtube/DaS/Vanilla/prompt.txt'):\n",
    "    #     rp.r._run_sys_command('python source/datasets/youtube/DaS/Vanilla/make_columns.py')\n",
    "    # if not rp.folder_exists('diffusion_shader_model_CKPT'):\n",
    "    #     rp.make_hardlink('diffusion_shader_model','diffusion_shader_model_CKPT',recursive=True)\n",
    "    \n",
    "    if \"pipe\" not in globals():\n",
    "        print(\"INITIALIZING PIPE\")\n",
    "        update_to_latest_checkpoint()\n",
    "        pipe = cogtrack.CogVideoXImageToVideoPipelineTracking.from_pretrained(\n",
    "            CKPT_folder,\n",
    "        )\n",
    "    \n",
    "    pipe.to(dtype=torch.bfloat16)\n",
    "    pipe.to(device)\n",
    "    #pipe.enable_sequential_cpu_offload(device=device)\n",
    "    pipe.vae.enable_slicing()\n",
    "    pipe.vae.enable_tiling()\n",
    "    pipe.transformer.eval()\n",
    "    pipe.text_encoder.eval()\n",
    "    pipe.vae.eval()\n",
    "    \n",
    "    ##########################\n",
    "    # MAIN\n",
    "    ##########################\n",
    "    \n",
    "    output_video = run_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_tracks(frames, track_frames):\n",
    "    rp.validate_tensor_shapes(\n",
    "        frames=\"numpy: T H W 3\",\n",
    "        track_frames=\"numpy: T H W 3\",\n",
    "    )\n",
    "    alpha = track_frames.max(-1, keepdims=True)\n",
    "    output = alpha * track_frames + (1 - alpha) * frames\n",
    "    return output\n",
    "\n",
    "\n",
    "def text_symbol(x):\n",
    "    return rp.pil_text_to_image(\n",
    "        x, font=\"DejaVuSerif\", size=200, color=\"white\", background_color=\"black\"\n",
    "    )\n",
    "\n",
    "@rp.globalize_locals\n",
    "def save_diffusion_results():\n",
    "    global video\n",
    "    \n",
    "    arrow_image = text_symbol(\"â†’\")\n",
    "    plus_image = text_symbol(\"+\")\n",
    "    approx_image = text_symbol(\"â‰ˆ\")\n",
    "    \n",
    "    video = rp.as_numpy_array(video)[:, :, :, :3]\n",
    "    \n",
    "    preview_video = rp.horizontally_concatenated_videos(\n",
    "        rp.labeled_images(\n",
    "            overlay_tracks(counter_video_frames, counter_tracking_frames),\n",
    "            \"Counterfactual Input\",\n",
    "            font='Arial',\n",
    "            size=20,\n",
    "        ),\n",
    "        [arrow_image],\n",
    "        rp.labeled_images(overlay_tracks(video, tracking_frames), \"Diffusion Output\", size=20,         font='Arial'),\n",
    "    \n",
    "        ##THESE DONT HAVE GROUND TRUTH\n",
    "        # [approx_image],\n",
    "        # rp.labeled_images(overlay_tracks(frames, tracking_frames), \"Ground Truth\"),\n",
    "        \n",
    "        origin=\"center\",\n",
    "    )\n",
    "    \n",
    "    preview_video = rp.labeled_images(preview_video, rp.line_join(checkpoint_root,prompt), size_by_lines=True)\n",
    "    preview_video = rp.labeled_images(preview_video, TITLE, size=30,position='top',text_color='green yellow', font='Arial')\n",
    "    \n",
    "    rp.display_video(preview_video)\n",
    "    \n",
    "    preview_video_path = 'untracked/inferblobs_outputs/'+rp.get_folder_name(checkpoint_root)+'__'+result_title+'.mp4'\n",
    "    preview_video_path = rp.get_unique_copy_path(preview_video_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    rp.make_parent_directory(preview_video_path)\n",
    "    rp.save_video_mp4(preview_video, preview_video_path, framerate=30, show_progress=False)\n",
    "    \n",
    "    \n",
    "    ic(preview_video_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## EDIT VIDEOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#WORKER\n",
    "seed = rp.millis()%10000 ; rp.seed_all(seed)\n",
    "\n",
    "input_video_path = \"MoveTheCar.mp4\"\n",
    "prompt = 'A minivan with a bunch of colorful baloons is driving through a dusty desert highway with power pylons in the top right of the screen'\n",
    "\n",
    "TITLE = f\"[Seed {seed}] Move the car faster forward\"\n",
    "\n",
    "init_input_video()\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=20\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "init_points = [\n",
    "    [0, 580, 300], #0 white ::  Car\n",
    "    [0, 680, 260], #1 red ::  Balloons\n",
    "    [0, 650, 365], #2 green ::  License Plate\n",
    "    [32, 600, 60], #3 blue ::  Road 1\n",
    "    # [0, 343, 185], #3 blue ::  Road 1\n",
    "    [27, 100, 200], #4 cyan ::  Left of road\n",
    "    [24, 600, 200], #5 magenta ::  Right dirt\n",
    "    [48, 400, 250], #6 yellow ::  \n",
    "    # [0, 100, 100], #7 gray ::  \n",
    "    # [0, 150, 150], #7 dark blue ::  \n",
    "    # [0, 200, 200], #7 dark green ::  \n",
    "    # [0, 250, 250], #7 dark red ::  \n",
    "    # [0, 300, 300], #7 dark cyan ::  \n",
    "    # [0, 350, 350], #7 dark magenta ::  \n",
    "    # [0, 400, 400], #7 dark yellow ::  \n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "# new_tracks = add_drift(new_tracks,  1, 3, dx=30, dy=0, t_origin=30, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks,  2, 5, 6, dx=0, dy=-5, t_origin=25)\n",
    "# new_tracks = add_drift(new_tracks, 5 , dx=-7, dy=20, t_origin=19, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=-5, dy=0, t_origin=11, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=5, dy=0, t_origin=38, do_before=False, do_after=True)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=0, dy=5, t_origin=38, do_before=False, do_after=True)\n",
    "new_tracks[10:,:3] = new_tracks[10:11,:3] #Freeze pos\n",
    "new_tracks = add_drift(new_tracks, 0,1,2 , dx=-5, dy=-4, t_origin=0, do_before=False, do_after=True)\n",
    "new_visibles[:,:3]=1\n",
    "new_tracks = zoom_tracks(new_tracks, 0, 1, 2, d_scale = .99, t_origin = 10)\n",
    "\n",
    "# for track_num in len(init_points):\n",
    "    \n",
    "\n",
    "# new_tracks = horz_mirror_tracks(new_tracks, 0, 1, 2, x_origin=450)\n",
    "\n",
    "# new_visibles[35:45,1]=0\n",
    "# new_tracks, new_visibles = reverse_tracks(new_tracks, new_visibles, 0,4)\n",
    "\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "rp.display_video(gridded_video(draw_arrows(draw_tracks(input_video, new_tracks, new_visibles), tracks, new_tracks, visibles, new_visibles)))\n",
    "\n",
    "init_blob_videos()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKER\n",
    "TITLE = \"Move the car to the other lane and make it go faster\"\n",
    "\n",
    "input_video_path = \"MoveTheCar.mp4\"\n",
    "prompt = 'A minivan with a bunch of colorful baloons is driving through a dusty desert highway with power pylons in the top right of the screen'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=10\n",
    "\n",
    "seed = rp.millis()%10000 ; rp.seed_all(seed)\n",
    "TITLE = f\"[Seed {seed}] {TITLE}\"\n",
    "\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "init_points = [\n",
    "    [0, 580, 300], #0 white ::  Car\n",
    "    # [0, 680, 260], #1 red ::  Balloons\n",
    "    # [0, 650, 365], #2 green ::  License Plate\n",
    "    [32, 600, 60], #3 blue ::  Road 1\n",
    "    # [0, 343, 185], #3 blue ::  Road 1\n",
    "    [27, 100, 200], #4 cyan ::  Left of road\n",
    "    [24, 600, 200], #5 magenta ::  Right dirt\n",
    "    [48, 400, 250], #6 yellow ::  \n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "# new_tracks = add_drift(new_tracks,  1, 3, dx=30, dy=0, t_origin=30, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks,  2, 5, 6, dx=0, dy=-5, t_origin=25)\n",
    "# new_tracks = add_drift(new_tracks, 5 , dx=-7, dy=20, t_origin=19, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=-5, dy=0, t_origin=11, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=5, dy=0, t_origin=38, do_before=False, do_after=True)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=0, dy=5, t_origin=38, do_before=False, do_after=True)\n",
    "new_tracks[10:,:1] = new_tracks[10:11,:1] #Freeze pos\n",
    "new_tracks = add_drift(new_tracks, 0 , dx=-5, dy=-4, t_origin=0, do_before=False, do_after=True)\n",
    "new_visibles[:,:1]=1\n",
    "new_tracks = zoom_tracks(new_tracks, 0, d_scale = .985, t_origin = 10)\n",
    "\n",
    "new_tracks = add_drift(new_tracks, 0 , dx=3.4, dy=0, t_origin=0, do_before=False, do_after=True)\n",
    "new_tracks = horz_mirror(new_tracks, 0, x_origin=450)\n",
    "new_visibles[:,1]=1\n",
    "# new_visibles[35:45,1]=0\n",
    "# new_tracks, new_visibles = reverse_tracks(new_tracks, new_visibles, 0,4)\n",
    "\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "rp.display_video(gridded_video(draw_arrows(draw_tracks(input_video, new_tracks, new_visibles), tracks, new_tracks, visibles, new_visibles)))\n",
    "\n",
    "init_blob_videos()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKER\n",
    "TITLE = \"Mirror the car to the other lane and make it go faster\"\n",
    "\n",
    "input_video_path = \"MoveTheCar.mp4\"\n",
    "prompt = 'A minivan with a bunch of colorful baloons is driving through a dusty desert highway with power pylons in the top right of the screen'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=50\n",
    "\n",
    "seed = rp.millis()%10000 ; rp.seed_all(seed)\n",
    "TITLE = f\"[Seed {seed}] {TITLE}\"\n",
    "\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "init_points = [\n",
    "    [0, 580, 300], #0 white ::  Car\n",
    "    [0, 680, 260], #1 red ::  Balloons\n",
    "    [0, 650, 365], #2 green ::  License Plate\n",
    "    [32, 600, 60], #3 blue ::  Road 1\n",
    "    # [0, 343, 185], #3 blue ::  Road 1\n",
    "    [27, 100, 200], #4 cyan ::  Left of road\n",
    "    [24, 600, 200], #5 magenta ::  Right dirt\n",
    "    [48, 400, 250], #6 yellow ::  \n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "# new_tracks = add_drift(new_tracks,  1, 3, dx=30, dy=0, t_origin=30, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks,  2, 5, 6, dx=0, dy=-5, t_origin=25)\n",
    "# new_tracks = add_drift(new_tracks, 5 , dx=-7, dy=20, t_origin=19, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=-5, dy=0, t_origin=11, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=5, dy=0, t_origin=38, do_before=False, do_after=True)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=0, dy=5, t_origin=38, do_before=False, do_after=True)\n",
    "new_tracks[10:,:3] = new_tracks[10:11,:3] #Freeze pos\n",
    "new_tracks = add_drift(new_tracks, 0,1,2 , dx=-5, dy=-4, t_origin=0, do_before=False, do_after=True)\n",
    "new_visibles[:,:3]=1\n",
    "new_tracks = zoom_tracks(new_tracks, 0, 1, 2, d_scale = .985, t_origin = 10)\n",
    "\n",
    "new_tracks = add_drift(new_tracks, 0,1,2 , dx=3.4, dy=0, t_origin=0, do_before=False, do_after=True)\n",
    "new_tracks = horz_mirror_origins(new_tracks, 0, 1, 2, x_origin=450)\n",
    "new_visibles[:,3]=1\n",
    "# new_visibles[35:45,1]=0\n",
    "# new_tracks, new_visibles = reverse_tracks(new_tracks, new_visibles, 0,4)\n",
    "\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "rp.display_video(gridded_video(draw_arrows(draw_tracks(input_video, new_tracks, new_visibles), tracks, new_tracks, visibles, new_visibles)))\n",
    "\n",
    "init_blob_videos()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#WORKER\n",
    "TITLE = \"Hot Air Baloons: Swap all three and make them rise\"\n",
    "\n",
    "input_video_path = \"MakeTheBaloonsMove.mp4\"\n",
    "prompt = 'Several hot air baloons rise through a beautiful grassy serene valley'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=50\n",
    "\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "init_points = [\n",
    "    [0, 230, 130], #0 white ::  Top baloon\n",
    "    [0, 550, 400], #1 red ::  Blue Baloon\n",
    "    [36, 100, 350], #2 green ::  Small Left Baloon\n",
    "    [8, 600, 230], #3 blue ::  Road 1\n",
    "    [46, 100, 260], #4 cyan ::  Left of road\n",
    "    [22, 500, 280], #5 magenta ::  Right dirt\n",
    "    [0, 115, 360], #6 yellow ::  \n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "seed = rp.millis()%10000 ; rp.seed_all(seed)\n",
    "TITLE = f\"[Seed {seed}] {TITLE}\"\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "# new_tracks = add_drift(new_tracks,  1, 3, dx=30, dy=0, t_origin=30, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks,  2, 5, 6, dx=0, dy=-5, t_origin=25)\n",
    "# new_tracks = add_drift(new_tracks, 5 , dx=-7, dy=20, t_origin=19, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=-5, dy=0, t_origin=11, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=5, dy=0, t_origin=38, do_before=False, do_after=True)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=0, dy=5, t_origin=38, do_before=False, do_after=True)\n",
    "# new_tracks[10:,:3] = new_tracks[10:11,:3] #Freeze pos\n",
    "new_tracks = add_drift(new_tracks, 0,1,2 , dx=0, dy=-2, t_origin=0, do_before=False, do_after=True)\n",
    "# new_tracks = speed_tracks(new_tracks, 3,4,5,6, factor=.5)\n",
    "new_tracks[:,[0,1,2]]=new_tracks[:,[1,2,0]]\n",
    "# new_visibles[:,:3]=1\n",
    "# new_tracks = zoom_tracks(new_tracks, 0, 1, 2, d_scale = .985, t_origin = 10)\n",
    "\n",
    "# new_tracks = add_drift(new_tracks, 0,1,2 , dx=3.4, dy=0, t_origin=0, do_before=False, do_after=True)\n",
    "# new_tracks = horz_mirror(new_tracks, 0, 1, 2, x_origin=450)\n",
    "# new_visibles[:,3]=1\n",
    "# new_visibles[35:45,1]=0\n",
    "# new_tracks, new_visibles = reverse_tracks(new_tracks, new_visibles, 0,4)\n",
    "\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "rp.display_video(gridded_video(draw_arrows(draw_tracks(input_video, new_tracks, new_visibles), tracks, new_tracks, visibles, new_visibles)))\n",
    "\n",
    "init_blob_videos()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#WORKER\n",
    "TITLE = \"Hot Air Baloons: Slow camera, make baloons rise\"\n",
    "\n",
    "input_video_path = \"MakeTheBaloonsMove.mp4\"\n",
    "prompt = 'Several hot air baloons rise through a beautiful grassy serene valley'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=50\n",
    "\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "init_points = [\n",
    "    [0, 230, 130], #0 white ::  Top baloon\n",
    "    [0, 550, 400], #1 red ::  Blue Baloon\n",
    "    [36, 100, 350], #2 green ::  Small Left Baloon\n",
    "    [8, 600, 230], #3 blue ::  Road 1\n",
    "    [46, 100, 260], #4 cyan ::  Left of road\n",
    "    [22, 500, 280], #5 magenta ::  Right dirt\n",
    "    [0, 115, 360], #6 yellow ::  \n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "seed = rp.millis()%10000 ; rp.seed_all(seed)\n",
    "TITLE = f\"[Seed {seed}] {TITLE}\"\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "# new_tracks = add_drift(new_tracks,  1, 3, dx=30, dy=0, t_origin=30, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks,  2, 5, 6, dx=0, dy=-5, t_origin=25)\n",
    "# new_tracks = add_drift(new_tracks, 5 , dx=-7, dy=20, t_origin=19, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=-5, dy=0, t_origin=11, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=5, dy=0, t_origin=38, do_before=False, do_after=True)\n",
    "# new_tracks = add_drift(new_tracks, 2 , dx=0, dy=5, t_origin=38, do_before=False, do_after=True)\n",
    "# new_tracks[10:,:3] = new_tracks[10:11,:3] #Freeze pos\n",
    "new_tracks = add_drift(new_tracks, 0,1,2 , dx=0, dy=-4, t_origin=0, do_before=False, do_after=True)\n",
    "new_tracks, new_visibles = speed_tracks(new_tracks,new_visibles, 3,4,5,6, factor=.5)\n",
    "# new_visibles[:,:3]=1\n",
    "# new_tracks = zoom_tracks(new_tracks, 0, 1, 2, d_scale = .985, t_origin = 10)\n",
    "\n",
    "# new_tracks = add_drift(new_tracks, 0,1,2 , dx=3.4, dy=0, t_origin=0, do_before=False, do_after=True)\n",
    "# new_tracks = horz_mirror(new_tracks, 0, 1, 2, x_origin=450)\n",
    "# new_visibles[:,3]=1\n",
    "# new_visibles[35:45,1]=0\n",
    "# new_tracks, new_visibles = reverse_tracks(new_tracks, new_visibles, 0,4)\n",
    "\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "rp.display_video(gridded_video(draw_arrows(draw_tracks(input_video, new_tracks, new_visibles), tracks, new_tracks, visibles, new_visibles)))\n",
    "\n",
    "init_blob_videos()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#WORKER\n",
    "TITLE = \"Sora Basketball: The ball goes into the hoop\"\n",
    "\n",
    "input_video_path = \"MakeItNotBounce.mp4\"\n",
    "prompt = 'A basketball gets thrown into a basketball hoop with a nice swish. The basketball goes through the hoop.'\n",
    "prompt = '''A sleek basketball arcs gracefully through the air, its orange surface gleaming under the gymnasium lights, as it approaches the hoop with precision. The ball makes a perfect swish, slicing through the net with a satisfying sound, the net fluttering gently in its wake. The moment is captured in slow motion, highlighting the ball's trajectory and the seamless integration of its path into the hoop, emphasizing the skill and finesse of the throw. The scene shows a vibrant outdoor playground under a clear blue sky. At the center of the image is a basketball hoop viewed from below, giving a dynamic perspective. In the background, there's a playground structure with an orange slide and climbing frame, all enclosed in a fenced area. '''\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "SEED = 6303\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "init_points = [\n",
    "    [0, 100, 100], #0 white ::  Tree\n",
    "    [0, 420, 290], #1 red ::  Left of rim\n",
    "    # [0, 420, 290], #1 red ::  Left of rim\n",
    "    [0, 600, 200], #2 green ::  Backstop\n",
    "    [39, 400, 100], #3 blue ::  Ball 2\n",
    "    [43, 700, 380], #4 cyan ::  Hidden Behind Backstop\n",
    "    [43, 360, 90], #5 magenta ::  Basketball 2\n",
    "    [22, 400, 400], #6 yellow ::  Playground area\n",
    "    # [0, 100, 400], #6 yellow ::  Playground area\n",
    "]\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "# new_tracks = add_drift(new_tracks,  1, 3, dx=30, dy=0, t_origin=30, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks,  2, 5, 6, dx=0, dy=-5, t_origin=25)\n",
    "new_tracks = add_drift(new_tracks, 3,5 , dx=-4, dy=10, t_origin=19, do_before=False)\n",
    "new_tracks = add_drift(new_tracks, 3,5 , dx=0, dy=20, t_origin=19, do_before=False)\n",
    "new_tracks = add_drift(new_tracks, 3,5 , dx=0, dy=-13, t_origin=23, do_before=False)\n",
    "new_tracks = add_drift(new_tracks, 3,5 , dx=0, dy=-9, t_origin=33, do_before=False)\n",
    "# new_tracks = add_drift(new_tracks, 0,1,2,3,4,5,6 , dx=0, dy=-5, t_origin=15, do_before=False)\n",
    "new_tracks = add_drift(new_tracks, 3,5 , dx=2, dy=0, t_origin=0, do_before=False)\n",
    "new_tracks = add_drift(new_tracks, 3,5 , dx=-2, dy=0, t_origin=22, do_before=False)\n",
    "new_visibles[18:32,1]=0 \n",
    "new_visibles[23:,6]=0 \n",
    "visibles[:,3]=1\n",
    "visibles[:,5]=1\n",
    "new_visibles[:,3]=1\n",
    "new_visibles[:,5]=1\n",
    "# new_tracks, new_visibles = reverse_tracks(new_tracks, new_visibles, 0,4)\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "rp.display_video(output_video, framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#WORKER\n",
    "TITLE = \"Motorcycle Chase: The motorcycle chases the car\"\n",
    "\n",
    "input_video_path = \"MakeMotorcycleChaseCar.mp4\"\n",
    "prompt = 'A morotcycle chases a red car on a busy highway, with the camera panning backwards with other cars surrounding it'\n",
    "prompt = 'A morotcycle chases a red car on a busy highway, with the camera panning backwards with other cars surrounding it'\n",
    "prompt = '''A sleek black motorcycle, engine roaring, weaves through dense traffic on a bustling highway, hot on the heels of a vibrant red sports car. The camera, mounted on a trailing vehicle, pans backward, capturing the intense chase. Surrounding cars, a mix of sedans and trucks in various colors, blur past, emphasizing the high speed. The rider, clad in a dark leather jacket and helmet, expertly navigates the tight gaps, while the red car ahead zips through the congestion. The scene is a thrilling blend of motion and urgency, with the highway's median and guardrails flashing by, underscoring the perilous pursuit.'''\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "init_points = [\n",
    "    [0, 320, 300], #0 white ::  Red Car\n",
    "    [16, 120, 260], #1 red ::  Motorcycle\n",
    "    [48, 100, 60], #2 green ::  Hidden Building\n",
    "    [48, 380, 160], #3 blue ::  Visible Building\n",
    "    [38, 600, 260], #4 cyan ::  Background Car Right Lane\n",
    "    [22, 60, 260], #5 magenta ::  White Van on Side of Road\n",
    "    [20, 500, 150], #6 yellow ::  A signpost\n",
    "\n",
    "\n",
    "    # [9, 280.0, 137.0],\n",
    "    # [19, 630.0, 380.0],\n",
    "    # [19, 612.0, 234.0],\n",
    "    # [36, 688.0, 199.0],\n",
    "    # [36, 37.0, 93.0],\n",
    "    # [36, 58.0, 28.0],\n",
    "    # [36, 51.0, 224.5],\n",
    "    # [36, 159.5, 197.0],\n",
    "    # [36, 272.0, 227.0],\n",
    "    # [36, 310.0, 227.0],\n",
    "    # [36, 363.0, 224.0],\n",
    "    # [36, 506.0, 227.0],\n",
    "    # [36, 448.0, 138.0],\n",
    "    # [7, 356.0, 143.0],\n",
    "    # [7, 27.0, 184.5],\n",
    "]\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "#Red car acceleration\n",
    "new_tracks = add_drift(new_tracks, 0, dx=0, dy=2, t_origin=0, do_before=False) \n",
    "new_tracks = add_drift(new_tracks, 0, dx=0, dy=2, t_origin=10, do_before=False) \n",
    "new_tracks = add_drift(new_tracks, 0, dx=0, dy=2, t_origin=20, do_before=False) \n",
    "\n",
    "#Motorcycle Chasing\n",
    "new_tracks = add_drift(new_tracks, 1, dx=0, dy=2, t_origin=10, do_before=False) \n",
    "new_tracks = add_drift(new_tracks, 1, dx=2, dy=0, t_origin=25, do_before=False) \n",
    "new_tracks = add_drift(new_tracks, 1, dx=2, dy=.5, t_origin=30, do_before=False) \n",
    "new_tracks = add_drift(new_tracks, 1, dx=2, dy=1, t_origin=35, do_before=False) \n",
    "new_tracks = add_drift(new_tracks, 1, dx=1, dy=1, t_origin=37, do_before=False) \n",
    "new_tracks = add_drift(new_tracks, 1, dx=1, dy=2, t_origin=40, do_before=False) \n",
    "new_tracks = add_drift(new_tracks, 1, dx=0, dy=2, t_origin=42, do_before=False) \n",
    "new_tracks = add_drift(new_tracks, 1, dx=0, dy=2, t_origin=44, do_before=False) \n",
    "new_tracks = add_drift(new_tracks, 1, dx=0, dy=2, t_origin=46, do_before=False) \n",
    "\n",
    "# new_tracks = add_drift(new_tracks, 0 , dx=0, dy=2, t_origin=30, do_before=False) \n",
    "# new_visibles[23:,6]=0 \n",
    "# visibles[:,5]=1\n",
    "# new_tracks, new_visibles = reverse_tracks(new_tracks, new_visibles, 0,4)\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#WORKER\n",
    "TITLE = \"Sora Basketball: Single Point Test\"\n",
    "\n",
    "input_video_path = \"MakeItNotBounce.mp4\"\n",
    "prompt = 'A basketball gets thrown into a basketball hoop with a nice swish. The basketball goes through the hoop.'\n",
    "prompt = '''A sleek basketball arcs gracefully through the air, its orange surface gleaming under the gymnasium lights, as it approaches the hoop with precision. The ball makes a perfect swish, slicing through the net with a satisfying sound, the net fluttering gently in its wake. The moment is captured in slow motion, highlighting the ball's trajectory and the seamless integration of its path into the hoop, emphasizing the skill and finesse of the throw. The scene shows a vibrant outdoor playground under a clear blue sky. At the center of the image is a basketball hoop viewed from below, giving a dynamic perspective. In the background, there's a playground structure with an orange slide and climbing frame, all enclosed in a fenced area. '''\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [ [9, 400, 100], [9, 300, 100] ],\n",
    "    [ [38, 350, 100], [38, 350, 300] ],\n",
    "    *[ [x]*2 for x in [\n",
    "            [0, 100, 100], \n",
    "            [18, 400, 400],\n",
    "            [18, 400, 100],\n",
    "            [44, 500, 200],\n",
    "        ]\n",
    "    ],\n",
    "\n",
    "    * [ [[rp.random_int(0,T-1), rp.random_int(0,W-1), rp.random_int(0,H-1)]] * 2 for _ in range(6)],\n",
    "]\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "visibles[:]=0\n",
    "new_visibles[:]=0\n",
    "\n",
    "temporal_radius=2\n",
    "\n",
    "for blob_index, (from_point, to_point) in enumerate(point_pairs):\n",
    "    b=blob_index\n",
    "    \n",
    "    t,x,y = from_point\n",
    "    for time_delta in range(-temporal_radius, temporal_radius+1):\n",
    "        if 0<=t+time_delta<T:\n",
    "            print(t,x,y,b)\n",
    "            tracks[t+time_delta, b, 0] = x\n",
    "            tracks[t+time_delta, b, 1] = y\n",
    "            visibles[t+time_delta, b] = 1\n",
    "        \n",
    "    t,x,y = to_point\n",
    "    for time_delta in range(-temporal_radius, temporal_radius+1):\n",
    "        if 0<=t+time_delta<T:\n",
    "            print(t,x,y,b)\n",
    "            new_tracks[t+time_delta, b, 0] = x\n",
    "            new_tracks[t+time_delta, b, 1] = y\n",
    "            new_visibles[t+time_delta, b] = 1\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#WORKER\n",
    "# del pipe\n",
    "\n",
    "TITLE = \"Boat: Single Point Test\"\n",
    "\n",
    "#Adjust this based on the loaded checkpoint\n",
    "# POINT_MODE = 'point2point'\n",
    "POINT_MODE = 'track2point'\n",
    "\n",
    "input_video_path = \"MoveTheBoatLeft.mp4\"\n",
    "prompt = 'A ferry boat glides gracefully in the water as the camera pans up'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[{POINT_MODE}: Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    # [[26, 369, 255], [26, 530, 202]],\n",
    "    # [[14, 358, 265], [14, 438, 253]],\n",
    "    # [[46, 405, 324], [46, 625, 261]],\n",
    "    # [[46, 244.0, 102.0], [46, 244.0, 102.0]],\n",
    "    # [[46, 90.5, 341.0], [46, 90.5, 341.0]],\n",
    "    # [[46, 83.0, 150.0], [46, 83.0, 150.0]],\n",
    "    # [[46, 625.5, 449.0], [46, 625.5, 449.0]],\n",
    "    # [[46, 614.0, 108.0], [46, 614.0, 108.0]],\n",
    "    # [[18, 563.0, 112.0], [18, 563.0, 112.0]],\n",
    "    # [[18, 135.0, 189.0], [18, 135.0, 189.0]],\n",
    "    # [[18, 413.0, 400.0], [18, 413.0, 400.0]],\n",
    "    # [[7, 157.0, 107.0], [7, 157.0, 107.0]],\n",
    "    # [[7, 146.0, 340.0], [7, 146.0, 340.0]],\n",
    "    # [[7, 556.0, 354.0], [7, 556.0, 354.0]],\n",
    "    # [[0, 54.0, 27.0], [0, 54.0, 27.0]],\n",
    "    # [[0, 129.0, 421.0], [0, 129.0, 421.0]],\n",
    "    # [[0, 640.0, 452.0], [0, 640.0, 452.0]],\n",
    "    # [[0, 656.0, 30.0], [0, 656.0, 30.0]]\n",
    "\n",
    "    # [[19, 374, 249], [19, 551, 259]],\n",
    "    # [[28, 385, 262], [28, 670, 262]],\n",
    "    # [[9, 372, 255], [9, 515, 259]],\n",
    "    # [[4, 341, 249], [4, 432, 245]],\n",
    "    # # [[1, 337.0, 251.0], [1, 337.0, 251.0]],\n",
    "    # # [[3, 143.0, 100.0], [3, 143.0, 100.0]],\n",
    "    # [[3, 653.0, 233.0], [3, 653.0, 233.0]],\n",
    "    # # [[8, 181.0, 382.0], [8, 181.0, 382.0]],\n",
    "    # # [[8, 213.5, 91.5], [8, 213.5, 91.5]],\n",
    "    # # [[8, 555.0, 48.0], [8, 555.0, 48.0]],\n",
    "    # # [[14, 572.0, 437.0], [14, 572.0, 437.0]],\n",
    "    # # [[14, 26.0, 18.0], [14, 26.0, 18.0]],\n",
    "    # [[32, 664.0, 39.0], [32, 664.0, 39.0]],\n",
    "    # # [[32, 172.0, 45.0], [32, 172.0, 45.0]],\n",
    "    # # [[32, 128.0, 420.0], [32, 128.0, 420.0]],\n",
    "    # # [[46, 200.0, 61.0], [46, 200.0, 61.0]],\n",
    "    # [[46, 666.0, 117.0], [46, 666.0, 117.0]],\n",
    "    # # [[46, 127.0, 144.0], [46, 127.0, 144.0]]\n",
    "\n",
    "    # [[0, 302.0, 366.0], [0, 302.0, 366.0]],\n",
    "    # [[35, 377, 267], [35, 531, 266]],\n",
    "    # [[45, 672.0, 82.0], [45, 672.0, 82.0]],\n",
    "    # [[45, 64.0, 143.0], [45, 64.0, 143.0]],\n",
    "    # [[9, 88.0, 203.0], [9, 88.0, 203.0]],\n",
    "    # [[4, 414.0, 452.0], [4, 414.0, 452.0]],\n",
    "    # [[26, 92.0, 124.0], [26, 92.0, 124.0]]\n",
    "\n",
    "\n",
    "\n",
    "    [[0, 302.0, 319.0], [0, 302.0, 319.0], [15, 514, 289], [30, 622.0, 273.0]],\n",
    "    [[30, 213.0, 54.0], [30, 213.0, 54.0]],\n",
    "    [[17, 108.0, 283.0], [17, 108.0, 283.0]],\n",
    "    [[17, 645.0, 47.0], [17, 645.0, 47.0]],\n",
    "    [[43, 110.0, 54.0], [43, 110.0, 54.0]],\n",
    "    [[43, 323.0, 145.0], [43, 323.0, 145.0]],\n",
    "    [[4, 182.0, 96.0], [4, 182.0, 96.0]],\n",
    "    # [[4, 39.0, 426.0], [4, 39.0, 426.0]],\n",
    "    # [[4, 610.0, 343.0], [4, 610.0, 343.0]],\n",
    "    # [[4, 597.0, 80.0], [4, 597.0, 80.0]]\n",
    "\n",
    "]\n",
    "\n",
    "# point_pairs = [[x[0]]*len(x) for x in point_pairs]#USE ONLY INPUT POINTS - A SANITY CHECK. SHOULD RETURN SAME VIDEO.\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "assert POINT_MODE in ['track2point', 'point2point']\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "temporal_radius=2\n",
    "\n",
    "frame2frame_indices = [0,1,2,3]\n",
    "frame2frame_indices = range(len(init_points)) #Make all of them single points\n",
    "\n",
    "for blob_index in frame2frame_indices:\n",
    "\n",
    "    if POINT_MODE=='point2point':\n",
    "        visibles[:,blob_index]=0\n",
    "\n",
    "    if 1:\n",
    "        new_visibles[:,blob_index]=0\n",
    "    \n",
    "    from_point = point_pairs[blob_index][0]\n",
    "    to_points = point_pairs[blob_index][1:]\n",
    "    b=blob_index\n",
    "\n",
    "    if POINT_MODE=='point2point':\n",
    "        t,x,y = from_point\n",
    "        for time_delta in range(-temporal_radius, temporal_radius+1):\n",
    "            if 0<=t+time_delta<T:\n",
    "                print(t,x,y,b)\n",
    "                tracks[t+time_delta, b, 0] = x\n",
    "                tracks[t+time_delta, b, 1] = y\n",
    "                visibles[t+time_delta, b] = 1\n",
    "\n",
    "    if 1:\n",
    "        for to_point in to_points:\n",
    "            t,x,y = to_point\n",
    "            for time_delta in range(-temporal_radius, temporal_radius+1):\n",
    "                if 0<=t+time_delta<T:\n",
    "                    print(t,x,y,b)\n",
    "                    new_tracks[t+time_delta, b, 0] = x\n",
    "                    new_tracks[t+time_delta, b, 1] = y\n",
    "                    new_visibles[t+time_delta, b] = 1\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#WORKER\n",
    "TITLE = \"Boat: Move Test\"\n",
    "\n",
    "input_video_path = \"MoveTheBoatLeft.mp4\"\n",
    "prompt = 'A ferry boat glides gracefully in the water as the camera pans up'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[0, 310.0, 336.0], [0, 310.0, 336.0]],#On boat\n",
    "    [[0, 334.0, 170.0], [0, 334.0, 170.0]],#On boat\n",
    "    [[0, 93.0, 84.0], [0, 93.0, 84.0]],#In background\n",
    "    [[0, 601.0, 91.0], [0, 601.0, 91.0]],#In background\n",
    "    [[17, 44.0, 15.0], [17, 44.0, 15.0]],#In background\n",
    "    [[33, 680.0, 41.0], [33, 680.0, 41.0]],#In background\n",
    "    [[44, 326.0, 45.0], [44, 326.0, 45.0]]#In background\n",
    "]\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "new_tracks = add_drift(new_tracks, 0,1 , dx=-3, dy=-3, t_origin=25, do_before=True)\n",
    "new_tracks = add_drift(new_tracks, 1 , dx=-3, dy=0, t_origin=25, do_before=True)\n",
    "new_tracks = add_drift(new_tracks, 0 , dx=1, dy=0, t_origin=25, do_before=False)\n",
    "\n",
    "# new_tracks = add_drift(new_tracks, 0,1 , dx=3, dy=4, t_origin=25, do_before=True)\n",
    "# new_tracks = add_drift(new_tracks, 1 , dx=0, dy=4, t_origin=25, do_before=True)\n",
    "\n",
    "zoom_tracks(new_tracks, 0, 1, d_scale = .985, t_origin = 25)\n",
    "new_tracks[:,:2,0]-=100\n",
    "\n",
    "new_tracks[:,2:]=new_tracks[:,2:].flip(0)\n",
    "new_visibles[:,2:]=new_visibles[:,2:].flip(0)\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#WORKER\n",
    "TITLE = \"Judge: Walk Out\"\n",
    "\n",
    "input_video_path = \"GetUpAndWalkOut.mp4\"\n",
    "prompt = 'A black judge woman in black robes walks into the room from the right and sits on a table, crossing her arms'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=8\n",
    "num_inference_steps=40\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "SEED = 5176\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[18, 237.0, 103.0], [18, 237.0, 103.0]],\n",
    "    [[2, 336.0, 343.0], [2, 336.0, 343.0]],\n",
    "    [[44, 128.0, 206.0], [44, 128.0, 206.0]],\n",
    "    [[44, 639.0, 196.0], [44, 639.0, 196.0]],\n",
    "    [[44, 402.0, 218.0], [44, 402.0, 218.0]],\n",
    "    [[44, 406.0, 336.0], [44, 406.0, 336.0]],\n",
    "    [[18, 156.0, 339.0], [18, 156.0, 339.0]],    \n",
    "    [[32, 375.0, 371.0], [32, 375.0, 371.0]],#Hands\n",
    "    [[32, 396.0, 310.0], [32, 396.0, 310.0]],#Hands\n",
    "    [[32, 427.0, 90.0], [32, 427.0, 90.0]],#Face\n",
    "    [[32, 400.0, 134.0], [32, 400.0, 134.0]],#Face\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "judge_indices = [0,4,5,6,7,8,9,10]\n",
    "\n",
    "new_tracks[:, judge_indices, 0] = W - new_tracks[:, judge_indices,  0] - 1 + 80 #MIRROR ALL POINTS\n",
    "new_tracks[:,[9,0]] = new_tracks[:,[0,9]]\n",
    "alpha = torch.tensor(rp.full_range(np.clip(np.linspace(0,1,49)-30/49+.1, 0, 1))).to(dtype=tracks.dtype, device=tracks.device)\n",
    "rp.line_graph(rp.as_numpy_array(alpha), title='alpha')\n",
    "new_tracks[:,judge_indices]=rp.blend(new_tracks[:,judge_indices], tracks[:,judge_indices], alpha[:,None,None])\n",
    "new_tracks[:,[9,0]]=rp.blend(new_tracks[:,[9,0]], tracks[:,[9,0]], alpha[:,None,None])\n",
    "new_tracks[:,[9,0]]=rp.blend(new_tracks[:,[9,0]], tracks[:,[9,0]], alpha[:,None,None])\n",
    "new_tracks[:,[9,0]]=rp.blend(new_tracks[:,[9,0]], tracks[:,[9,0]], alpha[:,None,None])\n",
    "new_tracks[:,[9,0]] = new_tracks[:,[0,9]]\n",
    "\n",
    "\n",
    "\n",
    "new_visibles[:,[2,3]] = new_visibles[:,[3,2]]\n",
    "\n",
    "\n",
    "# new_tracks = add_drift(new_tracks, 0,1 , dx=-3, dy=-3, t_origin=25, do_before=True)\n",
    "# new_tracks = add_drift(new_tracks, 1 , dx=-3, dy=0, t_origin=25, do_before=True)\n",
    "# new_tracks = add_drift(new_tracks, 0 , dx=1, dy=0, t_origin=25, do_before=False)\n",
    "\n",
    "# new_tracks = add_drift(new_tracks, 0,1 , dx=3, dy=4, t_origin=25, do_before=True)\n",
    "# new_tracks = add_drift(new_tracks, 1 , dx=0, dy=4, t_origin=25, do_before=True)\n",
    "\n",
    "# zoom_tracks(new_tracks, 0, 1, d_scale = .985, t_origin = 25)\n",
    "# new_tracks[:,:2,0]-=100\n",
    "\n",
    "# new_tracks[:,2:]=new_tracks[:,2:].flip(0)\n",
    "# new_visibles[:,2:]=new_visibles[:,2:].flip(0)\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_video(output_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#WORKER\n",
    "TITLE = \"Judge: Walk In From Right + Zoom\"\n",
    "\n",
    "input_video_path = \"GetUpAndWalkOut.mp4\"\n",
    "prompt = 'A black judge woman in black robes walks into the room from the right and sits on a table, crossing her arms'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=8\n",
    "num_inference_steps=50\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "SEED = 5176\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[18, 237.0, 103.0], [18, 237.0, 103.0]],\n",
    "    [[2, 336.0, 343.0], [2, 336.0, 343.0]],\n",
    "    [[44, 128.0, 206.0], [44, 128.0, 206.0]],\n",
    "    [[44, 639.0, 196.0], [44, 639.0, 196.0]],\n",
    "    [[44, 402.0, 218.0], [44, 402.0, 218.0]],\n",
    "    [[44, 406.0, 336.0], [44, 406.0, 336.0]],\n",
    "    [[18, 156.0, 339.0], [18, 156.0, 339.0]],    \n",
    "    [[32, 375.0, 371.0], [32, 375.0, 371.0]],#Hands\n",
    "    [[32, 396.0, 310.0], [32, 396.0, 310.0]],#Hands\n",
    "    [[32, 427.0, 90.0], [32, 427.0, 90.0]],#Face\n",
    "    [[32, 400.0, 134.0], [32, 400.0, 134.0]],#Face\n",
    "\n",
    "    [[14, 578.0, 230.0], [14, 578.0, 230.0]],\n",
    "    [[14, 658.0, 340.0], [14, 658.0, 340.0]],\n",
    "    [[14, 648.0, 185.0], [14, 648.0, 185.0]],\n",
    "    # [[14, 568.0, 387.0], [14, 568.0, 387.0]],\n",
    "    [[14, 285.0, 87.0], [14, 285.0, 87.0]],\n",
    "    # [[14, 285.0, 87.0], [14, 285.0, 87.0]],\n",
    "    [[14, 302.0, 273.0], [14, 302.0, 273.0]],\n",
    "    # [[14, 411.0, 177.0], [14, 411.0, 177.0]],\n",
    "    [[14, 188.0, 200.0], [14, 188.0, 200.0]],\n",
    "    # [[14, 364.0, 395.0], [14, 364.0, 395.0]],\n",
    "    [[24, 122.0, 299.0], [24, 122.0, 299.0]],\n",
    "    [[24, 107.0, 404.0], [24, 107.0, 404.0]],\n",
    "    # [[24, 389.0, 260.0], [24, 389.0, 260.0]],\n",
    "    [[24, 319.0, 342.0], [24, 319.0, 342.0]],\n",
    "    [[24, 329.0, 118.0], [24, 329.0, 118.0]],\n",
    "    [[24, 224.0, 292.0], [24, 224.0, 292.0]],\n",
    "    # [[24, 387.0, 71.0], [24, 387.0, 71.0]],\n",
    "    [[24, 38.0, 80.0], [24, 38.0, 80.0]],\n",
    "    [[24, 74.0, 148.0], [24, 74.0, 148.0]],\n",
    "    [[24, 153.0, 87.0], [24, 153.0, 87.0]]\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "judge_indices = [0,4,5,6,7,8,9,10   , 21, 20, 19]\n",
    "non_judge_indices = sorted(set(range(len(init_points)))-set(judge_indices))\n",
    "\n",
    "#Make her walk in from right\n",
    "# new_tracks[:, judge_indices, 0] = W - new_tracks[:, judge_indices,  0] - 1 + 80 #MIRROR ALL POINTS\n",
    "# new_tracks[:,[9,0]] = new_tracks[:,[0,9]]\n",
    "# alpha = torch.tensor(rp.full_range(np.clip(np.linspace(0,1,49)-30/49+.1, 0, 1))).to(dtype=tracks.dtype, device=tracks.device)\n",
    "# rp.line_graph(rp.as_numpy_array(alpha), title='alpha')\n",
    "# new_tracks[:,judge_indices]=rp.blend(new_tracks[:,judge_indices], tracks[:,judge_indices], alpha[:,None,None])\n",
    "# new_tracks[:,[9,0]]=rp.blend(new_tracks[:,[9,0]], tracks[:,[9,0]], alpha[:,None,None])\n",
    "# new_tracks[:,[9,0]]=rp.blend(new_tracks[:,[9,0]], tracks[:,[9,0]], alpha[:,None,None])\n",
    "# new_tracks[:,[9,0]]=rp.blend(new_tracks[:,[9,0]], tracks[:,[9,0]], alpha[:,None,None])\n",
    "# new_tracks[:,[9,0]] = new_tracks[:,[0,9]]\n",
    "# new_visibles[:,[2,3]] = new_visibles[:,[3,2]]\n",
    "\n",
    "#Zoom into center\n",
    "new_tracks[:,non_judge_indices] = new_tracks[:1,non_judge_indices] #Make the background not move\n",
    "deltas = new_tracks + 0 \n",
    "center_x = W//2\n",
    "center_y = H//2\n",
    "center_x = new_tracks[-1:,[9,0],0].mean(1,keepdim=True)\n",
    "center_y = new_tracks[-1:,[9,0],1].mean(1,keepdim=True)\n",
    "deltas[:,:,0] -= center_x\n",
    "deltas[:,:,1] -= center_y\n",
    "alpha = torch.tensor(np.linspace(0,1,49)).to(dtype=tracks.dtype, device=tracks.device)\n",
    "new_tracks[:,non_judge_indices] += deltas[:,non_judge_indices] * alpha[:,None,None] / 2\n",
    "\n",
    "\n",
    "\n",
    "# new_tracks = add_drift(new_tracks, 0,1 , dx=-3, dy=-3, t_origin=25, do_before=True)\n",
    "# new_tracks = add_drift(new_tracks, 1 , dx=-3, dy=0, t_origin=25, do_before=True)\n",
    "# new_tracks = add_drift(new_tracks, 0 , dx=1, dy=0, t_origin=25, do_before=False)\n",
    "\n",
    "# new_tracks = add_drift(new_tracks, 0,1 , dx=3, dy=4, t_origin=25, do_before=True)\n",
    "# new_tracks = add_drift(new_tracks, 1 , dx=0, dy=4, t_origin=25, do_before=True)\n",
    "\n",
    "# zoom_tracks(new_tracks, 0, 1, d_scale = .985, t_origin = 25)\n",
    "# new_tracks[:,:2,0]-=100\n",
    "\n",
    "# new_tracks[:,2:]=new_tracks[:,2:].flip(0)\n",
    "# new_visibles[:,2:]=new_visibles[:,2:].flip(0)\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#WORKER\n",
    "TITLE = \"Truck Before Cab [Old]\"\n",
    "\n",
    "input_video_path = \"truck_before_cab.mp4\"\n",
    "prompt = 'A yellow taxi cab SUV drives in front of a green truck and a hotdog stand move around in times square, NYC'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5176\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[13, 438.0, 170.0], [13, 438.0, 170.0]],#0\n",
    "    [[13, 333.0, 28.0], [13, 333.0, 28.0]],#1\n",
    "    [[31, 416.0, 102.0], [31, 416.0, 102.0]],#2\n",
    "    [[37, 663.0, 26.0], [37, 663.0, 26.0]],#3\n",
    "    [[20, 437.0, 103.0], [20, 437.0, 103.0]],#4\n",
    "    [[20, 89.0, 382.0], [20, 89.0, 382.0]],#5\n",
    "    [[48, 143.0, 38.0], [48, 143.0, 38.0]],#6\n",
    "    [[16, 500, 400]],#Concrete below taxi #7\n",
    "    [[0, 300, 200]],#Taxi Cab Mirror #8\n",
    "    [[10, 400, 200]],#Taxi Cab Mirror #8\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "# judge_indices = [0,4,5,6,7,8,9,10   , 21, 20, 19]\n",
    "# non_judge_indices = sorted(set(range(len(init_points)))-set(judge_indices))\n",
    "\n",
    "#Make her walk in from right\n",
    "# new_tracks[:, judge_indices, 0] = W - new_tracks[:, judge_indices,  0] - 1 + 80 #MIRROR ALL POINTS\n",
    "# new_tracks[:,[9,0]] = new_tracks[:,[0,9]]\n",
    "# alpha = torch.tensor(rp.full_range(np.clip(np.linspace(0,1,49)-30/49+.1, 0, 1))).to(dtype=tracks.dtype, device=tracks.device)\n",
    "# rp.line_graph(rp.as_numpy_array(alpha), title='alpha')\n",
    "# new_tracks[:,judge_indices]=rp.blend(new_tracks[:,judge_indices], tracks[:,judge_indices], alpha[:,None,None])\n",
    "# new_tracks[:,[9,0]]=rp.blend(new_tracks[:,[9,0]], tracks[:,[9,0]], alpha[:,None,None])\n",
    "# new_tracks[:,[9,0]]=rp.blend(new_tracks[:,[9,0]], tracks[:,[9,0]], alpha[:,None,None])\n",
    "# new_tracks[:,[9,0]]=rp.blend(new_tracks[:,[9,0]], tracks[:,[9,0]], alpha[:,None,None])\n",
    "# new_tracks[:,[9,0]] = new_tracks[:,[0,9]]\n",
    "# new_visibles[:,[2,3]] = new_visibles[:,[3,2]]\n",
    "\n",
    "# #Zoom into center\n",
    "# new_tracks[:,non_judge_indices] = new_tracks[:1,non_judge_indices] #Make the background not move\n",
    "# deltas = new_tracks + 0 \n",
    "# center_x = W//2\n",
    "# center_y = H//2\n",
    "# center_x = new_tracks[-1:,[9,0],0].mean(1,keepdim=True)\n",
    "# center_y = new_tracks[-1:,[9,0],1].mean(1,keepdim=True)\n",
    "# deltas[:,:,0] -= center_x\n",
    "# deltas[:,:,1] -= center_y\n",
    "# alpha = torch.tensor(np.linspace(0,1,49)).to(dtype=tracks.dtype, device=tracks.device)\n",
    "# new_tracks[:,non_judge_indices] += deltas[:,non_judge_indices] * alpha[:,None,None] / 2\n",
    "\n",
    "car = [0, 1, 8, 9]\n",
    "truck = [2, 3]\n",
    "\n",
    "# def retime(tracks, visibles, *i, old_start=0, old_end=-1, new_start=0, new_end=-1):\n",
    "\n",
    "        \n",
    "    \n",
    "new_tracks  [:,car]=resize_list_linterp(new_tracks  [:,car], T*2)[:T]\n",
    "new_visibles[:,car]=rp.resize_list     (new_visibles[:,car], T*2)[:T]\n",
    "\n",
    "new_tracks  [:,truck]=resize_list_linterp(new_tracks  [:,truck], T*2)[T:]\n",
    "new_visibles[:,truck]=rp.resize_list     (new_visibles[:,truck], T*2)[T:]\n",
    "\n",
    "new_visibles[22:35,[2]] = 0\n",
    "\n",
    "#Hot dog\n",
    "new_visibles[:,[4]] = 0\n",
    "\n",
    "#Concrete below cab\n",
    "new_visibles[:,[7]] = 0\n",
    "new_visibles[:10,[7]] = 1\n",
    "new_visibles[30:,[7]] = 1\n",
    "\n",
    "\n",
    "# new_tracks = add_drift(new_tracks, 0,1 , dx=-3, dy=-3, t_origin=25, do_before=True)\n",
    "# new_tracks = add_drift(new_tracks, 1 , dx=-3, dy=0, t_origin=25, do_before=True)\n",
    "# new_tracks = add_drift(new_tracks, 0 , dx=1, dy=0, t_origin=25, do_before=False)\n",
    "\n",
    "# new_tracks = add_drift(new_tracks, 0,1 , dx=3, dy=4, t_origin=25, do_before=True)\n",
    "# new_tracks = add_drift(new_tracks, 1 , dx=0, dy=4, t_origin=25, do_before=True)\n",
    "\n",
    "# zoom_tracks(new_tracks, 0, 1, d_scale = .985, t_origin = 25)\n",
    "# new_tracks[:,:2,0]-=100\n",
    "\n",
    "# new_tracks[:,2:]=new_tracks[:,2:].flip(0)\n",
    "# new_visibles[:,2:]=new_visibles[:,2:].flip(0)\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Truck Before Cab\"\n",
    "\n",
    "input_video_path = \"truck_before_cab.mp4\"\n",
    "prompt = 'A yellow SUV taxi cab SUV drives in front of a green truck and a hotdog stand move around in times square, NYC'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=13\n",
    "num_inference_steps=50\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[13, 438.0, 170.0], [13, 438.0, 170.0]],#0\n",
    "    [[13, 333.0, 28.0], [13, 333.0, 28.0]],#1\n",
    "    [[31, 416.0, 102.0], [31, 416.0, 102.0]],#2\n",
    "    [[37, 663.0, 26.0], [37, 663.0, 26.0]],#3\n",
    "    [[20, 437.0, 103.0], [20, 437.0, 103.0]],#4\n",
    "    [[20, 89.0, 382.0], [20, 89.0, 382.0]],#5\n",
    "    [[48, 143.0, 38.0], [48, 143.0, 38.0]],#6\n",
    "    [[16, 500, 400]],#Concrete below taxi #7\n",
    "    [[0, 300, 200]],#Taxi Cab Mirror #8\n",
    "    [[10, 400, 200]],#Taxi Cab Mirror #9\n",
    "    [[16, 600, 400]],#Ground 10\n",
    "    # [[36, 120, 300]],#Ground 11 #NOT NEEDED\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "car = [0, 1, 8, 9]\n",
    "truck = [2, 3]\n",
    "\n",
    "# def retime(tracks, visibles, *i, old_start=0, old_end=-1, new_start=0, new_end=-1):\n",
    "\n",
    "\n",
    "car_start=30\n",
    "new_tracks  [car_start:,car]=resize_list_linterp(new_tracks  [:,car], (49-car_start))[:T]\n",
    "new_visibles[car_start:,car]=rp.resize_list     (new_visibles[:,car], (49-car_start))[:T]\n",
    "new_visibles[:car_start,car] = 0\n",
    "\n",
    "new_tracks  [:,truck]=resize_list_linterp(new_tracks  [:,truck], T*2)[T:]\n",
    "new_visibles[:,truck]=rp.resize_list     (new_visibles[:,truck], T*2)[T:]\n",
    "\n",
    "# new_visibles[22:35,[2]] = 0\n",
    "\n",
    "\n",
    "#HOT DOG CART\n",
    "new_visibles[:,[4]] = 0\n",
    "new_visibles[:9,[4]] = 1\n",
    "\n",
    "def tween(tracks, i, txy0, txy1):\n",
    "    tracks = tracks + 0\n",
    "    \n",
    "    if isinstance(txy0, int): txy0 = [txy0, *tracks[txy0,i]]\n",
    "    if isinstance(txy1, int): txy1 = [txy1, *tracks[txy1,i]]\n",
    "    \n",
    "    t0, x0, y0 = txy0\n",
    "    t1, x1, y1 = txy1\n",
    "    for t in range(t0, t1+1):\n",
    "        a=rp.iblend(t, t0, t1)\n",
    "        x=rp.blend(x0, x1, a)\n",
    "        y=rp.blend(y0, y1, a)\n",
    "        tracks[t,i,0]=x\n",
    "        tracks[t,i,1]=y\n",
    "\n",
    "    return tracks\n",
    "\n",
    "new_tracks = tween(new_tracks, 4, [0, 560, 140], 14)\n",
    "new_visibles[32:37,2]=0\n",
    "\n",
    "\n",
    "\n",
    "#Concrete below cab\n",
    "new_visibles[:,[7,10]] = 0\n",
    "new_visibles[:28,[7]] = 1\n",
    "new_visibles[35:,[7]] = 1\n",
    "new_visibles[34:,[10]] = 1\n",
    "new_visibles[0:27,[10]] = 1\n",
    "new_tracks[:,[7,10]]=cotracker_tracks[:,[7,10]] #Cotracker is better under occlusions\n",
    "# new_visibles[36:,[11]] = 0\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Reverse Windmills\"\n",
    "\n",
    "input_video_path = \"reverse_windmill.mp4\"\n",
    "prompt = 'two spinning windmills. A serene peaceful landscape with spinning windmills'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=13\n",
    "num_inference_steps=30\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[0, 214.0, 179.0], [0, 214.0, 179.0]],#Windmill 1 Spoke 0\n",
    "    [[0, 257.0, 222.0], [0, 257.0, 222.0]],#Windmill 1 Spoke 1\n",
    "    [[0, 163.0, 222.0], [0, 163.0, 222.0]],#Windmill 1 Spoke 2\n",
    "    [[5, 161.0, 237.0], [5, 161.0, 237.0]],#Windmill 1 Spoke 3 \n",
    "    [[5, 211.0, 232.0], [5, 211.0, 232.0]],#Windmill 1 Center 4\n",
    "    [[5, 413.0, 173.0], [5, 413.0, 173.0]],#Windmill 2 Spoke 5\n",
    "    [[5, 499.0, 135.0], [5, 499.0, 135.0]],#Windmill 2 Spoke 6\n",
    "    [[18, 422.0, 263.0], [18, 422.0, 263.0]],#Windmill 2 Spoke 7\n",
    "    [[18, 428.0, 147.0], [18, 428.0, 147.0]],#Windmill 2 Spoke 8 \n",
    "    [[18, 489.0, 208.0], [18, 489.0, 208.0]],#Windmill 2 Center 9\n",
    "    [[0, 166.0, 374.0], [0, 166.0, 374.0]],#Background 1 10\n",
    "    [[0, 554.0, 379.0], [0, 554.0, 379.0]],#Background 2 11\n",
    "    [[48, 502.0, 129.0], [48, 502.0, 129.0]],#W2 12\n",
    "    [[48, 415.0, 177.0], [48, 415.0, 177.0]],#W2 13\n",
    "    [[48, 453.0, 275.0], [48, 453.0, 275.0]],#W2 14\n",
    "    [[48, 259.0, 209.0], [48, 259.0, 209.0]],#W1 15\n",
    "    [[48, 200.0, 180.0], [48, 200.0, 180.0]],#W1 16\n",
    "    [[48, 171.0, 246.0], [48, 171.0, 246.0]],#W1 17\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#BAD TRACKS\n",
    "# visibles[:,[16,13,14]]=0\n",
    "\n",
    "# q=3  ; visibles[:,q] = 0 ; visibles[:21, q ] = 1\n",
    "# q=5  ; visibles[:,q] = 0 ; visibles[:23, q ] = 1\n",
    "# # q=6  ; visibles[:,q] = 0 ; visibles[:13, q ] = 1\n",
    "# q=8  ; visibles[:,q] = 0 ; visibles[7:34, q ] = 1\n",
    "# # q=7  ; visibles[:,q] = 0 ; visibles[15:42, q ] = 1\n",
    "# q=12 ; visibles[:,q] = 0 ; visibles[28:, q] = 1\n",
    "# # q=15 ; visibles[:,q] = 0 ; visibles[:22, q] = 1\n",
    "# q=17 ; visibles[:,q] = 0 ; visibles[21:, q] = 1\n",
    "# q=13 ; visibles[:,q] = 0 ; visibles[:0 , q] = 1\n",
    "# q=14 ; visibles[:,q] = 0 ; visibles[:0 , q] = 1\n",
    "# q=16 ; visibles[:,q] = 0 ; visibles[:0 , q] = 1\n",
    "\n",
    "# print(\"SJA\",tracks.shape, visibles.shape)\n",
    "# tracks = list(tracks.permute(1,0,2))\n",
    "# visibles = list(visibles.permute(1,0))\n",
    "# to_delete = [13, 14, 16]\n",
    "# to_delete = sorted(to_delete, reverse=True)\n",
    "# for i in to_delete:\n",
    "#     print(\"DEL\",i)\n",
    "#     del tracks[i]\n",
    "#     del visibles[i]\n",
    "# tracks = torch.stack(tracks).permute(1,0,2)\n",
    "# visibles = torch.stack(visibles).permute(1,0)\n",
    "\n",
    "# tracks = cotracker_tracks\n",
    "# visibles = cotracker_visibles\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "# new_tracks = cotracker_tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "wind1=[0,1,2,3,4, 15, 16, 17]\n",
    "wind2=[5,6,7,8,9, 12, 13, 14]\n",
    "\n",
    "\n",
    "#Reverse windmill #2\n",
    "new_tracks  [:,wind2]=new_tracks  [:,wind2].flip(0)\n",
    "new_visibles[:,wind2]=new_visibles[:,wind2].flip(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# car = [0, 1, 8, 9]\n",
    "# truck = [2, 3]\n",
    "\n",
    "# def retime(tracks, visibles, *i, old_start=0, old_end=-1, new_start=0, new_end=-1):\n",
    "\n",
    "\n",
    "# car_start=30\n",
    "# new_tracks  [car_start:,car]=resize_list_linterp(new_tracks  [:,car], (49-car_start))[:T]\n",
    "# new_visibles[car_start:,car]=rp.resize_list     (new_visibles[:,car], (49-car_start))[:T]\n",
    "# new_visibles[:car_start,car] = 0\n",
    "\n",
    "# new_tracks  [:,truck]=resize_list_linterp(new_tracks  [:,truck], T*2)[T:]\n",
    "# new_visibles[:,truck]=rp.resize_list     (new_visibles[:,truck], T*2)[T:]\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### WORKER\n",
    "TITLE = \"[Failure] Stop Sign Lady\"\n",
    "\n",
    "input_video_path = \"stop_sign_lady.mp4\"\n",
    "prompt = 'a woman holds a sign saying \"STOP!\" and holds up a megaphone'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=13\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[27, 388.0, 15.0], [27, 388.0, 15.0]],\n",
    "    [[27, 412.0, 7.0], [27, 412.0, 7.0]],\n",
    "    [[48, 421.0, 157.0], [48, 421.0, 157.0]],\n",
    "    [[48, 397.0, 248.0], [48, 397.0, 248.0]],\n",
    "    [[48, 329.0, 178.0], [48, 329.0, 178.0]],\n",
    "    [[48, 315.0, 277.0], [48, 315.0, 277.0]],\n",
    "    [[48, 330.0, 423.0], [48, 330.0, 423.0]],\n",
    "    [[6, 298.0, 305.0], [6, 298.0, 305.0]],\n",
    "    # [[6, 227.0, 466.0], [6, 227.0, 466.0]],\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "# new_tracks = cotracker_tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "# car = [0, 1, 8, 9]\n",
    "# truck = [2, 3]\n",
    "\n",
    "# def retime(tracks, visibles, *i, old_start=0, old_end=-1, new_start=0, new_end=-1):\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "new_tracks = partial_drift(new_tracks, 0, 1, dx = 200, dy=300-60, t_start = 0, t_end=14)\n",
    "new_tracks = partial_drift(new_tracks, 2, dx = 100, dy=150-60, t_start = 0, t_end=14)\n",
    "new_tracks = partial_drift(new_tracks, 3, dx = 0, dy=0, t_start = 0, t_end=14)\n",
    "# new_tracks = partial_drift(new_tracks, 0, 3, dx = 5, dy=5, t_start = 0, t_end=14)\n",
    "new_tracks[:,[0,1],1]-=30\n",
    "\n",
    "# car_start=30\n",
    "# new_tracks  [car_start:,car]=resize_list_linterp(new_tracks  [:,car], (49-car_start))[:T]\n",
    "# new_visibles[car_start:,car]=rp.resize_list     (new_visibles[:,car], (49-car_start))[:T]\n",
    "# new_visibles[:car_start,car] = 0\n",
    "\n",
    "# new_tracks  [:,truck]=resize_list_linterp(new_tracks  [:,truck], T*2)[T:]\n",
    "# new_visibles[:,truck]=rp.resize_list     (new_visibles[:,truck], T*2)[T:]\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### WORKER\n",
    "TITLE = \"[Failure] Stop Sign Lady\"\n",
    "\n",
    "input_video_path = \"stop_sign_lady.mp4\"\n",
    "prompt = 'a woman holds a megaphone in one hand to the left a megaphone a sign saying \"STOP!\" and holds up a megaphone'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=13\n",
    "num_inference_steps=40\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "SEED = 4370\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[27, 388.0, 15.0], [27, 388.0, 15.0]],\n",
    "    [[27, 412.0, 7.0], [27, 412.0, 7.0]],\n",
    "    [[48, 421.0, 157.0], [48, 421.0, 157.0]],\n",
    "    [[48, 397.0, 248.0], [48, 397.0, 248.0]],\n",
    "    [[48, 329.0, 178.0], [48, 329.0, 178.0]],\n",
    "    [[48, 315.0, 277.0], [48, 315.0, 277.0]],\n",
    "    [[48, 330.0, 423.0], [48, 330.0, 423.0]],\n",
    "    [[6, 298.0, 305.0], [6, 298.0, 305.0]],\n",
    "    # [[6, 227.0, 466.0], [6, 227.0, 466.0]],\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "new_tracks, new_visibles = cotracker_tracks + 0, new_visibles+0\n",
    "\n",
    "# new_visibles[:,[6,7]]=1\n",
    "# new_visibles[:,[0,1,2]]=1 #Arm Sign\n",
    "# new_tracks = partial_drift(new_tracks, 5, dx = -200, dy=0, t_start = 0, t_end=30)\n",
    "new_tracks = partial_drift(new_tracks, 5, dx = -150, dy=0, t_start = 0, t_end=49)\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "# new_tracks = partial_drift(new_tracks, 0, 1, dx = 200, dy=300-60, t_start = 0, t_end=14)\n",
    "# new_tracks = partial_drift(new_tracks, 2, dx = 100, dy=150-60, t_start = 0, t_end=14)\n",
    "# new_tracks = partial_drift(new_tracks, 3, dx = 0, dy=0, t_start = 0, t_end=14)\n",
    "# new_tracks[:,[0,1],1]-=30\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Penguins Walk Together\"\n",
    "\n",
    "input_video_path = \"penguins_walk_together.mp4\"\n",
    "prompt = 'two penguins walk along a rocky beach shore'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=13\n",
    "num_inference_steps=50\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[0, 109.0, 249.0], [0, 109.0, 249.0]],\n",
    "    [[0, 379.0, 377.0], [0, 379.0, 377.0]],\n",
    "    [[0, 690.0, 285.0], [0, 690.0, 285.0]],\n",
    "    [[12, 326.0, 140.0], [12, 326.0, 140.0]],\n",
    "    [[12, 355.0, 239.0], [12, 355.0, 239.0]],\n",
    "    [[29, 278.0, 197.0], [29, 278.0, 197.0]],\n",
    "    [[29, 297.0, 270.0], [29, 297.0, 270.0]],#Pengui\n",
    "   # [[7, 569.0, 168.5], [7, 569.0, 168.5]],\n",
    "   #  [[7, 612.0, 163.0], [7, 612.0, 163.0]],\n",
    "   #  [[7, 597.0, 197.0], [7, 597.0, 197.0]],\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "# new_tracks = cotracker_tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "camera_points = [1]\n",
    "camera_deltas = tracks[:,camera_points].mean(1, keepdim=True)\n",
    "camera_deltas = camera_deltas - camera_deltas[:1]\n",
    "\n",
    "p1=[3,4,]#7,8,9]\n",
    "p2=[5,6]\n",
    "\n",
    "new_tracks  [:,p1] = resize_list_linterp((new_tracks-camera_deltas)[6:18, p1], T) + camera_deltas\n",
    "new_visibles[:,p1] = rp.resize_list     (new_visibles[6:18, p1], T)\n",
    "new_visibles[26:30,p2] = 0\n",
    "\n",
    "# new_tracks[:,p1]=new_tracks[14:15,p1]\n",
    "# new_visibles[:,p1]=1\n",
    "# new_tracks = partial_drift(new_tracks, *p1, dx = 10, dy=50, t_start = 0, t_end=14)\n",
    "# new_tracks = partial_drift(new_tracks, *p1, dx = 100, dy=160, t_start = 14, t_end=49)\n",
    "# new_tracks = partial_drift(new_tracks, 4, dx = -20, dy=30, t_start = 14, t_end=49) #Bottom of penguin\n",
    "# new_visibles[27:31,[5,6]] = 0 #Blocked by other penguin\n",
    "\n",
    "# new_tracks = partial_drift(new_tracks, 2, dx = 100, dy=150-60, t_start = 0, t_end=14)\n",
    "# new_tracks = partial_drift(new_tracks, 3, dx = 0, dy=0, t_start = 0, t_end=14)\n",
    "# # new_tracks = partial_drift(new_tracks, 0, 3, dx = 5, dy=5, t_start = 0, t_end=14)\n",
    "# new_tracks[:,[0,1],1]-=30\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Knight Chases Windmill\"\n",
    "\n",
    "input_video_path = \"chasing_windmills.mp4\"\n",
    "prompt = 'a knight chases a windmill'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[0, 234.0, 171.0], [0, 234.0, 171.0]], #Windmill Center\n",
    "    [[32, 245.0, 206.0], [32, 245.0, 206.0]],\n",
    "    [[32, 257.0, 123.0], [32, 257.0, 123.0]],\n",
    "    [[32, 145.0, 193.0], [32, 145.0, 193.0]],\n",
    "    [[32, 170.0, 101.0], [32, 170.0, 101.0]],\n",
    "    [[32, 333.0, 158.0], [32, 333.0, 158.0]],\n",
    "    [[32, 140.0, 323.0], [32, 140.0, 323.0]],\n",
    "    [[32, 443.0, 260.0], [32, 443.0, 260.0]],\n",
    "    [[32, 340.0, 274.0], [32, 340.0, 274.0]],\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "# new_tracks = cotracker_tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "camera_points = [0]\n",
    "camera_deltas = tracks[:,camera_points].mean(1, keepdim=True)\n",
    "camera_deltas = camera_deltas - camera_deltas[:1]\n",
    "\n",
    "w=[1,2,3,4]\n",
    "new_tracks  [:,w] = (new_tracks[:,w] - camera_deltas).flip(0) + camera_deltas\n",
    "\n",
    "# new_tracks[:,p1]=new_tracks[14:15,p1]\n",
    "# new_visibles[:,p1]=1\n",
    "# new_tracks = partial_drift(new_tracks, *p1, dx = 10, dy=50, t_start = 0, t_end=14)\n",
    "# new_tracks = partial_drift(new_tracks, *p1, dx = 100, dy=160, t_start = 14, t_end=49)\n",
    "# new_tracks = partial_drift(new_tracks, 4, dx = -20, dy=30, t_start = 14, t_end=49) #Bottom of penguin\n",
    "# new_visibles[27:31,[5,6]] = 0 #Blocked by other penguin\n",
    "\n",
    "# new_tracks = partial_drift(new_tracks, 2, dx = 100, dy=150-60, t_start = 0, t_end=14)\n",
    "# new_tracks = partial_drift(new_tracks, 3, dx = 0, dy=0, t_start = 0, t_end=14)\n",
    "# # new_tracks = partial_drift(new_tracks, 0, 3, dx = 5, dy=5, t_start = 0, t_end=14)\n",
    "# new_tracks[:,[0,1],1]-=30\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Knight Chases Windmill [Slower]\"\n",
    "\n",
    "input_video_path = \"chasing_windmills_slower.mp4\"\n",
    "prompt = 'a knight chases a windmill'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[0, 229.0, 169.0], [0, 229.0, 169.0]],\n",
    "\n",
    "    [[29, 254.0, 144.0], [29, 254.0, 144.0]],\n",
    "    [[29, 186.0, 111.0], [29, 186.0, 111.0]],\n",
    "    [[29, 147.0, 180.0], [29, 147.0, 180.0]],\n",
    "    [[20, 267.0, 186.0], [20, 267.0, 186.0]],\n",
    "    \n",
    "    [[0, 338.0, 140.0], [0, 338.0, 140.0]],\n",
    "    [[20, 171.0, 296.0], [20, 171.0, 296.0]],\n",
    "    [[20, 540.0, 229.0], [20, 540.0, 229.0]]\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "# new_tracks = cotracker_tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "camera_points = [0]\n",
    "camera_deltas = tracks[:,camera_points].mean(1, keepdim=True)\n",
    "camera_deltas = camera_deltas - camera_deltas[:1]\n",
    "\n",
    "w=[1,2,3,4]\n",
    "new_tracks  [:,w] = (new_tracks[:,w] - camera_deltas).flip(0) + camera_deltas\n",
    "\n",
    "# new_tracks[:,p1]=new_tracks[14:15,p1]\n",
    "# new_visibles[:,p1]=1\n",
    "# new_tracks = partial_drift(new_tracks, *p1, dx = 10, dy=50, t_start = 0, t_end=14)\n",
    "# new_tracks = partial_drift(new_tracks, *p1, dx = 100, dy=160, t_start = 14, t_end=49)\n",
    "# new_tracks = partial_drift(new_tracks, 4, dx = -20, dy=30, t_start = 14, t_end=49) #Bottom of penguin\n",
    "# new_visibles[27:31,[5,6]] = 0 #Blocked by other penguin\n",
    "\n",
    "# new_tracks = partial_drift(new_tracks, 2, dx = 100, dy=150-60, t_start = 0, t_end=14)\n",
    "# new_tracks = partial_drift(new_tracks, 3, dx = 0, dy=0, t_start = 0, t_end=14)\n",
    "# # new_tracks = partial_drift(new_tracks, 0, 3, dx = 5, dy=5, t_start = 0, t_end=14)\n",
    "# new_tracks[:,[0,1],1]-=30\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Shakycam\"\n",
    "\n",
    "input_video_path = \"shakycam.mp4\"\n",
    "prompt = 'a muddy puddle river next to a bunch of trees'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=12\n",
    "num_inference_steps=50\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[0, 162.0, 78.0], [0, 162.0, 78.0]],\n",
    "    [[0, 619.0, 342.0], [0, 619.0, 342.0]],\n",
    "    [[42, 633.0, 61.0], [42, 633.0, 61.0]],\n",
    "    [[42, 207.0, 100.0], [42, 207.0, 100.0]],\n",
    "    [[42, 478.0, 371.0], [42, 478.0, 371.0]],\n",
    "    [[48, 660.0, 135.0], [48, 660.0, 135.0]],\n",
    "    [[5, 328.0, 117.0], [5, 328.0, 117.0]],\n",
    "    [[48, 342.0, 114.0], [48, 342.0, 114.0]],\n",
    "    [[48, 480.0, 350.0], [48, 480.0, 350.0]],\n",
    "    [[48, 143.0, 291.0], [48, 143.0, 291.0]],\n",
    "    [[48, 469.0, 132.0], [48, 469.0, 132.0]],\n",
    "    [[15, 501.0, 55.0], [15, 501.0, 55.0]],\n",
    "    [[15, 565.0, 226.0], [15, 565.0, 226.0]],\n",
    "    [[15, 441.0, 298.0], [15, 441.0, 298.0]],\n",
    "    [[15, 460.0, 414.0], [15, 460.0, 414.0]],\n",
    "    [[15, 158.0, 151.0], [15, 158.0, 151.0]],\n",
    "    [[15, 288.0, 76.5], [15, 288.0, 76.5]],\n",
    "    [[15, 232.0, 306.0], [15, 232.0, 306.0]],\n",
    "    [[15, 77.0, 332.0], [15, 77.0, 332.0]],\n",
    "    [[3, 135.0, 118.0], [3, 135.0, 118.0]],\n",
    "    [[3, 104.0, 324.0], [3, 104.0, 324.0]],\n",
    "    [[3, 351.0, 405.0], [3, 351.0, 405.0]],\n",
    "    [[3, 277.0, 73.0], [3, 277.0, 73.0]],\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_tracks = cotracker_tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "new_tracks = (new_tracks[:-5] + new_tracks[1:-4] + new_tracks[2:-3]  + new_tracks[3:-2] + new_tracks[4:-1] +  new_tracks[5:]) / 5\n",
    "new_tracks = resize_list_linterp(new_tracks.contiguous(), 49)\n",
    "\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Arms Up Spinny\"\n",
    "\n",
    "input_video_path = \"ArmsUp.mp4\"\n",
    "prompt = 'a woman in a grape field spins around and raises her arms up'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=12\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[21, 469.5, 458.0], [21, 469.5, 458.0]],\n",
    "    [[21, 431.0, 126.0], [21, 431.0, 126.0]],\n",
    "    [[21, 547.0, 205.0], [21, 547.0, 205.0]],\n",
    "    [[21, 69.0, 67.0], [21, 69.0, 67.0]],\n",
    "    [[33, 432.0, 112.0], [33, 432.0, 112.0]],\n",
    "    [[33, 396.0, 336.0], [33, 396.0, 336.0]],\n",
    "    [[33, 262.0, 366.0], [33, 262.0, 366.0]],\n",
    "    [[33, 132.0, 358.0], [33, 132.0, 358.0]],\n",
    "    \n",
    "    [[20, 600, 420], [33, 132.0, 358.0]]\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "visibles[:,0]=1\n",
    "new_tracks = partial_drift(new_tracks, 0, dy=-100, t_start = 0, t_end=25)\n",
    "\n",
    "new_tracks[:,[8]] = cotracker_tracks[:,[8]] + 0\n",
    "visibles[:,[8]]=1\n",
    "\n",
    "\n",
    "\n",
    "# new_tracks = (new_tracks[:-5] + new_tracks[1:-4] + new_tracks[2:-3]  + new_tracks[3:-2] + new_tracks[4:-1] +  new_tracks[5:]) / 5\n",
    "# new_tracks = resize_list_linterp(new_tracks.contiguous(), 49)\n",
    "\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Arms Up Spinny\"\n",
    "\n",
    "input_video_path = \"ArmsUp.mp4\"\n",
    "prompt = 'a woman in a grape field spins around and raises her arms up'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=12\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[21, 469.5, 458.0], [21, 469.5, 458.0]],\n",
    "    [[21, 431.0, 126.0], [21, 431.0, 126.0]],\n",
    "    [[21, 547.0, 205.0], [21, 547.0, 205.0]],\n",
    "    [[21, 69.0, 67.0], [21, 69.0, 67.0]],\n",
    "    [[33, 432.0, 112.0], [33, 432.0, 112.0]],\n",
    "    [[33, 396.0, 336.0], [33, 396.0, 336.0]],\n",
    "    [[33, 262.0, 366.0], [33, 262.0, 366.0]],\n",
    "    [[33, 132.0, 358.0], [33, 132.0, 358.0]],\n",
    "    \n",
    "    [[20, 600, 420], [33, 132.0, 358.0]]\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "visibles[:,0]=1\n",
    "new_tracks = partial_drift(new_tracks, 0,1,2,3,4,5,6,7 , dy=-300, t_start = 0, t_end=49)\n",
    "\n",
    "# new_tracks[:,[8]] = cotracker_tracks[:,[8]] + 0\n",
    "# visibles[:,[8]]=1\n",
    "\n",
    "\n",
    "\n",
    "# new_tracks = (new_tracks[:-5] + new_tracks[1:-4] + new_tracks[2:-3]  + new_tracks[3:-2] + new_tracks[4:-1] +  new_tracks[5:]) / 5\n",
    "# new_tracks = resize_list_linterp(new_tracks.contiguous(), 49)\n",
    "\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Candle Grab\"\n",
    "\n",
    "input_video_path = \"candle_grab.MOV\"\n",
    "prompt = 'an arm reaches out and grabs a jar yankee candle jar on a couch with two candles on it in jars'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=12\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[28, 549.0, 189.0], [28, 549.0, 189.0]],\n",
    "    [[28, 261.0, 258.0], [28, 261.0, 258.0]],\n",
    "    [[28, 411.0, 346.0], [28, 411.0, 346.0]],\n",
    "    [[32, 607.0, 363.0], [32, 607.0, 363.0]],\n",
    "    [[32, 396.0, 205.0], [32, 396.0, 205.0]],\n",
    "    [[32, 213.0, 242.0], [32, 213.0, 242.0]],\n",
    "    [[32, 178.0, 419.0], [32, 178.0, 419.0]],\n",
    "    [[32, 124.0, 68.0], [32, 124.0, 68.0]],\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "visibles[:]=1\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "tracks[:,3]=cotracker_tracks[:,3]\n",
    "\n",
    "visibles[:,0]=1\n",
    "visibles[:26,3]=0\n",
    "new_tracks = partial_drift(new_tracks, 0,1,2,3,4,5,6,7 , dy=100, dx=-100, t_start = 0, t_end=49)\n",
    "# new_tracks = partial_drift(new_tracks, 2,3,4,5 , dx=320, dy=-100, t_start = 0, t_end=36)\n",
    "\n",
    "\n",
    "# new_tracks[:,[8]] = cotracker_tracks[:,[8]] + 0\n",
    "# visibles[:,[8]]=1\n",
    "\n",
    "\n",
    "\n",
    "# new_tracks = (new_tracks[:-5] + new_tracks[1:-4] + new_tracks[2:-3]  + new_tracks[3:-2] + new_tracks[4:-1] +  new_tracks[5:]) / 5\n",
    "# new_tracks = resize_list_linterp(new_tracks.contiguous(), 49)\n",
    "\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Candle Grab StopCam\"\n",
    "\n",
    "input_video_path = \"candle_grab.MOV\"\n",
    "prompt = 'an arm reaches out and grabs a jar yankee candle jar on a couch with two candles on it in jars'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=12\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[28, 549.0, 189.0], [28, 549.0, 189.0]],\n",
    "    [[28, 261.0, 258.0], [28, 261.0, 258.0]],\n",
    "    [[28, 411.0, 346.0], [28, 411.0, 346.0]],\n",
    "    [[32, 607.0, 363.0], [32, 607.0, 363.0]],\n",
    "    [[32, 396.0, 205.0], [32, 396.0, 205.0]],\n",
    "    [[32, 213.0, 242.0], [32, 213.0, 242.0]],\n",
    "    [[32, 178.0, 419.0], [32, 178.0, 419.0]],\n",
    "    [[32, 124.0, 68.0], [32, 124.0, 68.0]],\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "visibles[:]=1\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "tracks[:,3]=cotracker_tracks[:,3]\n",
    "\n",
    "visibles[:,0]=1\n",
    "visibles[:26,3]=0\n",
    "ref_points = [0,7,6,1]\n",
    "\n",
    "\n",
    "# new_tracks = partial_drift(new_tracks, 0,1,2,3,4,5,6,7 , dy=100, dx=-100, t_start = 0, t_end=49)\n",
    "# new_tracks = partial_drift(new_tracks, 2,3,4,5 , dx=320, dy=-100, t_start = 0, t_end=36)\n",
    "new_tracks = uncamera(new_tracks, new_tracks[:,ref_points], origin_frame=25)\n",
    "\n",
    "# new_tracks[:,[8]] = cotracker_tracks[:,[8]] + 0\n",
    "# visibles[:,[8]]=1\n",
    "\n",
    "\n",
    "\n",
    "# new_tracks = (new_tracks[:-5] + new_tracks[1:-4] + new_tracks[2:-3]  + new_tracks[3:-2] + new_tracks[4:-1] +  new_tracks[5:]) / 5\n",
    "# new_tracks = resize_list_linterp(new_tracks.contiguous(), 49)\n",
    "\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Cheerleader\"\n",
    "\n",
    "input_video_path = \"Cheerleader2.mp4\"\n",
    "prompt = 'a cheerleader raises up a red pompom happily with energy in a gym'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=12\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[40, 150.0, 218.0], [40, 150.0, 218.0]],\n",
    "    [[40, 243.0, 360.0], [40, 243.0, 360.0]],\n",
    "    [[40, 346.0, 417.0], [40, 346.0, 417.0]],\n",
    "    [[40, 439.0, 223.0], [40, 439.0, 223.0]],\n",
    "    [[40, 447.0, 345.0], [40, 447.0, 345.0]],\n",
    "    [[40, 342.0, 119.0], [40, 342.0, 119.0]],\n",
    "    [[40, 470.0, 461.0], [40, 470.0, 461.0]]\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "new_visibles[:,1]=1\n",
    "visibles[:,1]=1\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "new_tracks=partial_drift(new_tracks,0,1,dy=-160,dx=-80,t_end=30)\n",
    "new_tracks=partial_drift(new_tracks,0,dy=60,dx=160,t_end=49,t_start=30)\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Cheerleader Two\"\n",
    "\n",
    "input_video_path = \"Cheerleader1.mp4\"\n",
    "prompt = 'a cheerleader raises up a red pompom happily with energy in a gym'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=12\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "SEED = 4409\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[42, 269.0, 365.0], [42, 269.0, 365.0]],\n",
    "    [[42, 461.0, 369.0], [42, 461.0, 369.0]],\n",
    "    [[42, 366.0, 144.0], [42, 366.0, 144.0]],\n",
    "    [[42, 370.0, 315.0], [42, 370.0, 315.0]],\n",
    "    # [[42, 417.0, 215.0], [42, 417.0, 215.0]],\n",
    "    # [[42, 319.0, 216.5], [42, 319.0, 216.5]],\n",
    "    [[2, 225.0, 466.0], [2, 225.0, 466.0]],\n",
    "    [[2, 663.0, 463.0], [2, 663.0, 463.0]],\n",
    "\n",
    "    [[43, 668.0, 335.0], [43, 668.0, 335.0]],\n",
    "    [[43, 19.0, 360.0], [43, 19.0, 360.0]]\n",
    "]\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "new_visibles[:,1]=1\n",
    "visibles[:,1]=1\n",
    "\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "new_tracks=partial_drift(new_tracks,1,dy=-300,dx=30,t_start=20,t_end=41)\n",
    "# new_tracks=partial_drift(new_tracks,0,1,dy=-300,dx=0,t_start=20,t_end=41)\n",
    "# new_tracks=partial_drift(new_tracks,0,dy=60,dx=160,t_end=49,t_start=30)\n",
    "new_tracks[:,[0,1,2,3,6],0]+=5\n",
    "\n",
    "def restore_tracks(new_tracks, tracks,  *i, t_start, t_end):\n",
    "    for j in i:\n",
    "        dx,dy=tracks[t_end,j] - new_tracks[t_end,j]\n",
    "        new_tracks = partial_drift(new_tracks, j, dx=dx,dy=dy, t_start=t_start, t_end=t_end)\n",
    "    return new_tracks\n",
    "\n",
    "new_tracks = restore_tracks(new_tracks, tracks, 2,4,5,6,7, t_start=40, t_end=48)\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WORKER\n",
    "TITLE = \"Kids Racing\"\n",
    "\n",
    "input_video_path = \"kids_racing.mp4\"\n",
    "prompt = 'kids racing and running on a road with trees on either side and the girl in the red shirt pulls ahead and wins the race'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=12\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 5072\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs = [\n",
    "    [[21, 267.0, 275.0], [21, 267.0, 275.0]],\n",
    "    [[21, 521.0, 181.0], [21, 521.0, 181.0]],\n",
    "    [[21, 397.0, 226.0], [21, 397.0, 226.0]],\n",
    "    [[21, 549.0, 71.0], [21, 549.0, 71.0]],\n",
    "    [[21, 143.0, 120.0], [21, 143.0, 120.0]],\n",
    "    [[21, 288.0, 386.0], [21, 288.0, 386.0]],\n",
    "    [[21, 516.0, 243.0], [21, 516.0, 243.0]],\n",
    "    # [[21, 435.0, 200.0], [21, 435.0, 200.0]],\n",
    "    # [[21, 368.0, 184.0], [21, 368.0, 184.0]],\n",
    "    # [[21, 322.0, 189.0], [21, 322.0, 189.0]],\n",
    "    # [[21, 272.0, 189.0], [21, 272.0, 189.0]]\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "new_visibles[:,1]=1\n",
    "visibles[:,1]=1\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "# new_tracks=partial_drift(new_tracks,0,1,dy=-160,dx=-80,t_end=30)\n",
    "# new_tracks=partial_drift(new_tracks,0,dy=60,dx=160,t_end=49,t_start=30)\n",
    "\n",
    "# new_tracks  [:,[0,5]] = resize_list_linterp(new_tracks  [:,[0,5]], 100)[:T]\n",
    "# new_visibles[:,[0,5]] = rp.resize_list     (new_visibles[:,[0,5]], 100)[:T]\n",
    "\n",
    "FASTKID=[1,6]\n",
    "new_tracks  [:,[FASTKID]] = resize_list_linterp(new_tracks  [:,[FASTKID]], 100)[:T]\n",
    "new_visibles[:,[FASTKID]] = rp.resize_list     (new_visibles[:,[FASTKID]], 100)[:T]\n",
    "\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # del pipe\n",
    "\n",
    "# #### WORKER\n",
    "# TITLE = \"Splash\"\n",
    "\n",
    "# input_video_path = \"spash_slow.mp4\"\n",
    "# prompt = 'kids jumping in a pool and the dad reaches out to catch him as he splashes'\n",
    "\n",
    "# latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "# guidance_scale=12\n",
    "# num_inference_steps=39\n",
    "\n",
    "# SEED = rp.millis() % 9999\n",
    "# # SEED = 5072\n",
    "\n",
    "# rp.seed_all(SEED)\n",
    "# TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "# init_input_video()\n",
    "\n",
    "# ##############################\n",
    "\n",
    "# #In TXY form\n",
    "\n",
    "# point_pairs =[\n",
    "#     [[0, 135.0, 156.0], [0, 135.0, 156.0]],\n",
    "#     # [[0, 135.0, 234.0], [0, 135.0, 234.0]],\n",
    "#     [[0, 324.0, 189.0], [0, 324.0, 189.0]],\n",
    "#     # [[0, 336.0, 273.0], [0, 336.0, 273.0]],\n",
    "#     [[0, 444.5, 173.0+40], [0, 444.5, 173.0]],\n",
    "#     [[0, 447.0, 277.0], [0, 447.0, 277.0]],\n",
    "#     [[30, 312.0, 381.0], [30, 312.0, 381.0]],\n",
    "#     # [[30, 149.0, 324.0], [30, 149.0, 324.0]],\n",
    "#     [[30, 616.0, 110.0], [30, 616.0, 110.0]],\n",
    "#     # [[30, 100.0, 114.0-40], [30, 109.0, 114.0]]\n",
    "# ]\n",
    "\n",
    "\n",
    "# init_points = [\n",
    "#     x[0] for x in point_pairs\n",
    "# ]\n",
    "\n",
    "# K1=[0]\n",
    "# K2=[1]#3\n",
    "# K3=[2,3,4]#5\n",
    "# CAM=[5,6]\n",
    "\n",
    "# rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "# init_tracks()\n",
    "\n",
    "# ##############################\n",
    "\n",
    "# #Made without the good tracker\n",
    "# visibles = cotracker_visibles\n",
    "# tracks = cotracker_tracks\n",
    "\n",
    "# new_tracks = tracks + 0\n",
    "# new_visibles = visibles + 0\n",
    "\n",
    "# new_visibles[:,1]=1\n",
    "# visibles[:,1]=1\n",
    "\n",
    "\n",
    "\n",
    "# def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "#     t_delta = t_end-t_start\n",
    "#     tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "#     tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "#     return tracks\n",
    "\n",
    "# # new_tracks=partial_drift(new_tracks,0,1,dy=-160,dx=-80,t_end=30)\n",
    "# # new_tracks=partial_drift(new_tracks,0,dy=60,dx=160,t_end=49,t_start=30)\n",
    "\n",
    "# # new_tracks  [:,[0,5]] = resize_list_linterp(new_tracks  [:,[0,5]], 100)[:T]\n",
    "# # new_visibles[:,[0,5]] = rp.resize_list     (new_visibles[:,[0,5]], 100)[:T]\n",
    "\n",
    "\n",
    "# new_tracks = uncamera(new_tracks, tracks[:,CAM])\n",
    "\n",
    "# # new_tracks  [:,K3] = resize_list_linterp(new_tracks  [:,K3], 100)[:T]\n",
    "# # new_visibles[:,K3] = rp.resize_list     (new_visibles[:,K3], 100)[:T]\n",
    "\n",
    "# new_tracks  [:,K3+K1] = new_tracks  [:1,K3+K1]\n",
    "# new_visibles[:,K3+K1] = new_visibles[:1,K3+K1]\n",
    "\n",
    "# new_tracks  [:,K2] = resize_list_linterp(new_tracks  [20:,K2], T)\n",
    "# new_visibles[:,K2] = rp.resize_list     (new_visibles[20:,K2], T)\n",
    "\n",
    "# new_tracks = recamera(new_tracks, tracks[:,CAM])\n",
    "\n",
    "# rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "# display_tracks_diff()\n",
    "\n",
    "# ##############################\n",
    "\n",
    "# init_blob_videos()\n",
    "# init_sample()\n",
    "# init_mp4_files()\n",
    "# do_diffusion()\n",
    "# save_diffusion_results()\n",
    "# display_tracks_diff(output_video[:,:,:,:3])\n",
    "# rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe\n",
    "\n",
    "#### WORKER\n",
    "TITLE = \"Kittycat Fish\"\n",
    "\n",
    "input_video_path = \"KittyFish.mp4\"\n",
    "prompt = 'the cat turns around and walks away. a cat watches a goldfish in a bowl as the goldfish swims around. the cat then gets up and walks away. The cat walks away. the cat gets up and walks away from the fish.'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=16\n",
    "num_inference_steps=30\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "SEED = 2\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs =[\n",
    "    [[0, 441.0, 176.0], [0, 441.0, 176.0]],\n",
    "    [[0, 95.0, 308.0], [0, 95.0, 308.0]],\n",
    "    [[0, 58.0, 400.5], [0, 58.0, 400.5]],\n",
    "    [[0, 522.0, 294.0], [0, 522.0, 294.0]],\n",
    "    [[6, 219.0, 275.0], [6, 219.0, 275.0]],\n",
    "    [[22, 254.0, 210.0], [22, 254.0, 210.0]],\n",
    "    [[39, 315.0, 251.0], [39, 315.0, 251.0]],\n",
    "    [[17, 136.0, 205.0], [17, 136.0, 205.0]]\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "# K1=[0]\n",
    "# K2=[1]#3\n",
    "# K3=[2,3,4]#5\n",
    "# CAM=[5,6]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker - keep things repeatable\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "new_visibles[:,1]=1\n",
    "visibles[:,1]=1\n",
    "\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "# new_tracks=partial_drift(new_tracks,0,1,dy=-160,dx=-80,t_end=30)\n",
    "# new_tracks=partial_drift(new_tracks,0,dy=60,dx=160,t_end=49,t_start=30)\n",
    "\n",
    "new_tracks = partial_drift(new_tracks, 0, dx=100, t_start=0, t_end=T)\n",
    "new_tracks = partial_drift(new_tracks, 0,3, dx=200, t_start=20, t_end=T)\n",
    "\n",
    "# new_tracks  [:,[0,5]] = resize_list_linterp(new_tracks  [:,[0,5]], 100)[:T]\n",
    "# new_visibles[:,[0,5]] = rp.resize_list     (new_visibles[:,[0,5]], 100)[:T]\n",
    "\n",
    "\n",
    "# new_tracks = uncamera(new_tracks, tracks[:,CAM])\n",
    "\n",
    "# new_tracks  [:,K3] = resize_list_linterp(new_tracks  [:,K3], 100)[:T]\n",
    "# new_visibles[:,K3] = rp.resize_list     (new_visibles[:,K3], 100)[:T]\n",
    "\n",
    "# new_tracks  [:,K3+K1] = new_tracks  [:1,K3+K1]\n",
    "# new_visibles[:,K3+K1] = new_visibles[:1,K3+K1]\n",
    "\n",
    "# new_tracks  [:,K2] = resize_list_linterp(new_tracks  [20:,K2], T)\n",
    "# new_visibles[:,K2] = rp.resize_list     (new_visibles[20:,K2], T)\n",
    "\n",
    "# new_tracks = recamera(new_tracks, tracks[:,CAM])\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe\n",
    "\n",
    "#### WORKER\n",
    "TITLE = \"Bichon + Corgi : Bichon Stays Behind\"\n",
    "\n",
    "input_video_path = \"Borgi.mp4\"\n",
    "prompt = 'A bichon frise stays behind, while a corgi runs on the sidewalk, chasing a bone.'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=16\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 2\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs =[\n",
    "    [[0, 215.0, 171.0], [0, 215.0, 171.0]],\n",
    "    [[0, 430.0, 302.0], [0, 430.0, 302.0]],\n",
    "    [[48, 180.0, 152.0], [48, 180.0, 152.0]],\n",
    "    [[48, 375.0, 432.0], [48, 375.0, 432.0]],\n",
    "    [[48, 340.0, 180.0], [48, 340.0, 180.0]],\n",
    "    [[14, 612.0, 160.0], [14, 612.0, 160.0]],\n",
    "    [[14, 650.0, 292.0], [14, 650.0, 292.0]]\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "# K1=[0]\n",
    "# K2=[1]#3\n",
    "# K3=[2,3,4]#5\n",
    "# CAM=[5,6]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker - keep things repeatable\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "new_tracks += torch.randn_like(new_tracks) * 2\n",
    "\n",
    "new_visibles[:,1]=1\n",
    "visibles[:,1]=1\n",
    "\n",
    "new_visibles[25:,[2]]=0\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "# new_tracks=partial_drift(new_tracks,0,1,dy=-160,dx=-80,t_end=30)\n",
    "# new_tracks=partial_drift(new_tracks,0,dy=60,dx=160,t_end=49,t_start=30)\n",
    "\n",
    "# new_tracks = partial_drift(new_tracks, 0, dx=100, t_start=0, t_end=T)\n",
    "# new_tracks = partial_drift(new_tracks, 0,3, dx=200, t_start=20, t_end=T)\n",
    "new_tracks = partial_drift(new_tracks, 0, dx=-200, dy=-200, t_start=25, t_end=T)\n",
    "\n",
    "# new_tracks  [:,[0,5]] = resize_list_linterp(new_tracks  [:,[0,5]], 100)[:T]\n",
    "# new_visibles[:,[0,5]] = rp.resize_list     (new_visibles[:,[0,5]], 100)[:T]\n",
    "\n",
    "\n",
    "# new_tracks = uncamera(new_tracks, tracks[:,CAM])\n",
    "\n",
    "# new_tracks  [:,K3] = resize_list_linterp(new_tracks  [:,K3], 100)[:T]\n",
    "# new_visibles[:,K3] = rp.resize_list     (new_visibles[:,K3], 100)[:T]\n",
    "\n",
    "# new_tracks  [:,K3+K1] = new_tracks  [:1,K3+K1]\n",
    "# new_visibles[:,K3+K1] = new_visibles[:1,K3+K1]\n",
    "\n",
    "# new_tracks  [:,K2] = resize_list_linterp(new_tracks  [20:,K2], T)\n",
    "# new_visibles[:,K2] = rp.resize_list     (new_visibles[20:,K2], T)\n",
    "\n",
    "# new_tracks = recamera(new_tracks, tracks[:,CAM])\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe\n",
    "\n",
    "#### WORKER\n",
    "TITLE = \"Bichon + Corgi : Bichon Stay Behind\"\n",
    "\n",
    "input_video_path = \"Borgi.mp4\"\n",
    "prompt = 'A bichon frise and a corgi run on the sidewalk, chasing a bone. the bichon breaks to the left.'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=16\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "SEED = 7945\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs =[\n",
    "    [[0, 215.0, 171.0], [0, 215.0, 171.0]],\n",
    "    [[0, 430.0, 302.0], [0, 430.0, 302.0]],\n",
    "    [[48, 180.0, 152.0], [48, 180.0, 152.0]],\n",
    "    [[48, 375.0, 432.0], [48, 375.0, 432.0]],\n",
    "    [[48, 340.0, 180.0], [48, 340.0, 180.0]],\n",
    "    [[14, 612.0, 160.0], [14, 612.0, 160.0]],\n",
    "    [[14, 650.0, 292.0], [14, 650.0, 292.0]],\n",
    "\n",
    "    # [[41, 363.0, 450.0], [41, 363.0, 450.0]],\n",
    "    # [[34, 383.0, 447.0], [34, 383.0, 447.0]],\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "# K1=[0]\n",
    "# K2=[1]#3\n",
    "# K3=[2,3,4]#5\n",
    "# CAM=[5,6]\n",
    "\n",
    "STAY = [0]\n",
    "CAM=[2,5,6]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker - keep things repeatable\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "new_tracks += torch.randn_like(new_tracks) * 3.5\n",
    "\n",
    "new_visibles[:,1]=1\n",
    "visibles[:,1]=1\n",
    "\n",
    "new_visibles[25:,[2]]=0\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "# new_tracks=partial_drift(new_tracks,0,1,dy=-160,dx=-80,t_end=30)\n",
    "# new_tracks=partial_drift(new_tracks,0,dy=60,dx=160,t_end=49,t_start=30)\n",
    "\n",
    "# new_tracks = partial_drift(new_tracks, 0, dx=100, t_start=0, t_end=T)\n",
    "# new_tracks = partial_drift(new_tracks, 0,3, dx=200, t_start=20, t_end=T)\n",
    "# new_tracks = partial_drift(new_tracks, 0, dx=-200, dy=-200, t_start=25, t_end=T)\n",
    "# \n",
    "# new_tracks  [:,[0,5]] = resize_list_linterp(new_tracks  [:,[0,5]], 100)[:T]\n",
    "# new_visibles[:,[0,5]] = rp.resize_list     (new_visibles[:,[0,5]], 100)[:T]\n",
    "\n",
    "\n",
    "new_tracks[:,STAY] = uncamera(new_tracks[:,STAY], tracks[:,CAM])\n",
    "new_tracks[:,STAY] = rp.blend(new_tracks[:1,STAY], new_tracks[:,STAY], 0)\n",
    "new_tracks[:,STAY] = recamera(new_tracks[:,STAY], tracks[:,CAM])\n",
    "\n",
    "# new_tracks  [:,K3] = resize_list_linterp(new_tracks  [:,K3], 100)[:T]\n",
    "# new_visibles[:,K3] = rp.resize_list     (new_visibles[:,K3], 100)[:T]\n",
    "\n",
    "# new_tracks  [:,K3+K1] = new_tracks  [:1,K3+K1]\n",
    "# new_visibles[:,K3+K1] = new_visibles[:1,K3+K1]\n",
    "\n",
    "# new_tracks  [:,K2] = resize_list_linterp(new_tracks  [20:,K2], T)\n",
    "# new_visibles[:,K2] = rp.resize_list     (new_visibles[20:,K2], T)\n",
    "\n",
    "# new_tracks = recamera(new_tracks, tracks[:,CAM])\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe\n",
    "\n",
    "#### WORKER\n",
    "TITLE = \"Bichon + Corgi : Corgi Stay Behind\"\n",
    "\n",
    "input_video_path = \"Borgi.mp4\"\n",
    "prompt = 'A bichon frise and a corgi run on the sidewalk, chasing a bone. the bichon breaks to the left.'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=16\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 7945\n",
    "SEED = 9995\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs =[\n",
    "    [[0, 215.0, 171.0], [0, 215.0, 171.0]],\n",
    "    [[0, 430.0, 302.0], [0, 430.0, 302.0]],\n",
    "    [[48, 180.0, 152.0], [48, 180.0, 152.0]],\n",
    "    [[48, 375.0, 432.0], [48, 375.0, 432.0]],\n",
    "    [[48, 340.0, 180.0], [48, 340.0, 180.0]],\n",
    "    [[14, 612.0, 160.0], [14, 612.0, 160.0]],\n",
    "    [[14, 650.0, 292.0], [14, 650.0, 292.0]],\n",
    "\n",
    "#     [[41, 363.0, 450.0], [41, 363.0, 450.0]],\n",
    "#     [[34, 383.0, 447.0], [34, 383.0, 447.0]],\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "# K1=[0]\n",
    "# K2=[1]#3\n",
    "# K3=[2,3,4]#5\n",
    "# CAM=[5,6]\n",
    "\n",
    "STAY = [1]\n",
    "CAM=[2,5,6]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker - keep things repeatable\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "new_tracks += torch.randn_like(new_tracks) * 1\n",
    "\n",
    "new_visibles[:,1]=1\n",
    "visibles[:,1]=1\n",
    "\n",
    "new_visibles[25:,[2]]=0\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "# new_tracks=partial_drift(new_tracks,0,1,dy=-160,dx=-80,t_end=30)\n",
    "# new_tracks=partial_drift(new_tracks,0,dy=60,dx=160,t_end=49,t_start=30)\n",
    "\n",
    "# new_tracks = partial_drift(new_tracks, 0, dx=100, t_start=0, t_end=T)\n",
    "# new_tracks = partial_drift(new_tracks, 0,3, dx=200, t_start=20, t_end=T)\n",
    "# new_tracks = partial_drift(new_tracks, 0, dx=-200, dy=-200, t_start=25, t_end=T)\n",
    "# \n",
    "# new_tracks  [:,[0,5]] = resize_list_linterp(new_tracks  [:,[0,5]], 100)[:T]\n",
    "# new_visibles[:,[0,5]] = rp.resize_list     (new_visibles[:,[0,5]], 100)[:T]\n",
    "\n",
    "\n",
    "new_tracks[:,STAY] = uncamera(new_tracks[:,STAY], tracks[:,CAM])\n",
    "new_tracks[:,STAY] = rp.blend(new_tracks[:1,STAY], new_tracks[:,STAY], 0)\n",
    "new_tracks[:,STAY] = recamera(new_tracks[:,STAY], tracks[:,CAM])\n",
    "\n",
    "# new_tracks  [:,K3] = resize_list_linterp(new_tracks  [:,K3], 100)[:T]\n",
    "# new_visibles[:,K3] = rp.resize_list     (new_visibles[:,K3], 100)[:T]\n",
    "\n",
    "# new_tracks  [:,K3+K1] = new_tracks  [:1,K3+K1]\n",
    "# new_visibles[:,K3+K1] = new_visibles[:1,K3+K1]\n",
    "\n",
    "# new_tracks  [:,K2] = resize_list_linterp(new_tracks  [20:,K2], T)\n",
    "# new_visibles[:,K2] = rp.resize_list     (new_visibles[20:,K2], T)\n",
    "\n",
    "# new_tracks = recamera(new_tracks, tracks[:,CAM])\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe\n",
    "\n",
    "#### WORKER\n",
    "TITLE = \"Blacks Swan Go Faster\"\n",
    "\n",
    "input_video_path = \"blackswan.mp4\"\n",
    "prompt = ''\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=6\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 7945\n",
    "SEED = 9995\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs =[\n",
    "    [[0, 180.0, 305.0], [0, 180.0, 305.0]],\n",
    "    [[0, 416.0, 316.0], [0, 416.0, 316.0]],\n",
    "    [[0, 364.0, 98.0], [0, 364.0, 98.0]],\n",
    "    [[0, 541.0, 18.0], [0, 541.0, 18.0]],\n",
    "    [[25, 361.0, 44.0], [25, 361.0, 44.0]],\n",
    "    [[42, 423.0, 37.0], [42, 423.0, 37.0]],\n",
    "    [[48, 612.0, 39.0], [48, 612.0, 39.0]]\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "# K1=[0]\n",
    "# K2=[1]#3\n",
    "# K3=[2,3,4]#5\n",
    "# CAM=[5,6]\n",
    "\n",
    "STAY = [1]\n",
    "CAM=[3]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker - keep things repeatable\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "new_tracks += torch.randn_like(new_tracks) * 1\n",
    "\n",
    "new_visibles[:,1]=1\n",
    "visibles[:,1]=1\n",
    "\n",
    "# new_visibles[25:,[2]]=0\n",
    "\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "new_tracks=partial_drift(new_tracks,0,1,2,dy=-60,dx=300,t_end=T)\n",
    "# new_tracks=partial_drift(new_tracks,0,dy=60,dx=160,t_end=49,t_start=30)\n",
    "\n",
    "# new_tracks = partial_drift(new_tracks, 0, dx=100, t_start=0, t_end=T)\n",
    "# new_tracks = partial_drift(new_tracks, 0,3, dx=200, t_start=20, t_end=T)\n",
    "# new_tracks = partial_drift(new_tracks, 0, dx=-200, dy=-200, t_start=25, t_end=T)\n",
    "# \n",
    "# new_tracks  [:,[0,5]] = resize_list_linterp(new_tracks  [:,[0,5]], 100)[:T]\n",
    "# new_visibles[:,[0,5]] = rp.resize_list     (new_visibles[:,[0,5]], 100)[:T]\n",
    "\n",
    "\n",
    "# new_tracks[:,STAY] = uncamera(new_tracks[:,STAY], tracks[:,CAM])\n",
    "# new_tracks[:,STAY] = rp.blend(new_tracks[:1,STAY], new_tracks[:,STAY], 0)\n",
    "# new_tracks[:,STAY] = recamera(new_tracks[:,STAY], tracks[:,CAM])\n",
    "\n",
    "# new_tracks  [:,K3] = resize_list_linterp(new_tracks  [:,K3], 100)[:T]\n",
    "# new_visibles[:,K3] = rp.resize_list     (new_visibles[:,K3], 100)[:T]\n",
    "\n",
    "# new_tracks  [:,K3+K1] = new_tracks  [:1,K3+K1]\n",
    "# new_visibles[:,K3+K1] = new_visibles[:1,K3+K1]\n",
    "\n",
    "# new_tracks  [:,K2] = resize_list_linterp(new_tracks  [20:,K2], T)\n",
    "# new_visibles[:,K2] = rp.resize_list     (new_visibles[20:,K2], T)\n",
    "\n",
    "# new_tracks = recamera(new_tracks, tracks[:,CAM])\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe\n",
    "\n",
    "#### WORKER\n",
    "TITLE = \"Blacks Freeze Camera\"\n",
    "\n",
    "input_video_path = \"blackswan.mp4\"\n",
    "prompt = 'A black swan swims through a river'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=3\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 7945\n",
    "# SEED = 9995\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs =[\n",
    "    [[0, 180.0, 305.0], [0, 180.0, 305.0]],\n",
    "    [[0, 416.0, 316.0], [0, 416.0, 316.0]],\n",
    "    [[0, 364.0, 98.0], [0, 364.0, 98.0]],\n",
    "    [[0, 541.0, 18.0], [0, 541.0, 18.0]],\n",
    "    [[25, 361.0, 44.0], [25, 361.0, 44.0]],\n",
    "    [[42, 423.0, 37.0], [42, 423.0, 37.0]],\n",
    "    [[48, 612.0, 39.0], [48, 612.0, 39.0]],\n",
    "    [[9, 650.5, 268.5], [9, 650.5, 268.5]],\n",
    "    [[19, 631.0, 124.0], [19, 631.0, 124.0]],\n",
    "    [[30, 297.0, 163.0], [30, 297.0, 163.0]],\n",
    "    [[30, 64.0, 194.0], [30, 64.0, 194.0]],\n",
    "    [[30, 646.0, 112.0], [30, 646.0, 112.0]],\n",
    "    [[40, 641.0, 213.0], [40, 641.0, 213.0]],\n",
    "    [[40, 273.0, 450.0], [40, 273.0, 450.0]],\n",
    "    [[40, 165.0, 252.0], [40, 165.0, 252.0]],\n",
    "    [[40, 44.0, 352.0], [40, 44.0, 352.0]],\n",
    "    [[40, 495.0, 310.0], [40, 495.0, 310.0]],\n",
    "    [[16, 511.0, 335.0], [16, 511.0, 335.0]],\n",
    "    [[16, 603.0, 421.0], [16, 603.0, 421.0]],\n",
    "    [[16, 78.0, 449.0], [16, 78.0, 449.0]],\n",
    "    [[16, 307.0, 326.0], [16, 307.0, 326.0]],\n",
    "    [[16, 219.0, 369.0], [16, 219.0, 369.0]],\n",
    "    [[16, 371.0, 350.0], [16, 371.0, 350.0]],\n",
    "    [[16, 448.5, 301.5], [16, 448.5, 301.5]],\n",
    "    [[16, 417.0, 224.0], [16, 417.0, 224.0]],\n",
    "    [[16, 399.0, 142.0], [16, 399.0, 142.0]],\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "SWAN = [0,1,2,20,21,22,23,24,25]\n",
    "BG=[3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "CAM=[3]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker - keep things repeatable\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "new_visibles[:,1]=1\n",
    "visibles[:,1]=1\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "new_tracks[:,:] = uncamera(new_tracks[:,:], tracks[:,CAM], T-1)\n",
    "\n",
    "new_tracks[:,BG] = new_tracks[-1:,BG]\n",
    "# new_visibles[:,BG]=1\n",
    "\n",
    "# new_tracks += torch.randn_like(new_tracks) * 1\n",
    "\n",
    "# new_tracks[:,SWAN] = rp.blend(new_tracks[:1,SWAN], new_tracks[:,STAY], 0)\n",
    "# new_tracks[:,SWAN] = recamera(new_tracks[:,SWAN], tracks[:,CAM])\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe\n",
    "\n",
    "#### WORKER\n",
    "TITLE = \"Blacks Freeze Camera\"\n",
    "\n",
    "input_video_path = \"blackswan.mp4\"\n",
    "prompt = 'A black swan swims through a river'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=0\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 7945\n",
    "# SEED = 9995\n",
    "SEED = 1515\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs =[\n",
    "    [[0, 180.0, 305.0], [0, 180.0, 305.0]],\n",
    "    [[0, 416.0, 316.0], [0, 416.0, 316.0]],\n",
    "    [[0, 364.0, 98.0], [0, 364.0, 98.0]],\n",
    "    [[0, 541.0, 18.0], [0, 541.0, 18.0]],\n",
    "    [[25, 361.0, 44.0], [25, 361.0, 44.0]],\n",
    "    [[42, 423.0, 37.0], [42, 423.0, 37.0]],\n",
    "    [[48, 612.0, 39.0], [48, 612.0, 39.0]],\n",
    "    [[9, 650.5, 268.5], [9, 650.5, 268.5]],\n",
    "    [[19, 631.0, 124.0], [19, 631.0, 124.0]],\n",
    "    [[30, 297.0, 163.0], [30, 297.0, 163.0]],\n",
    "    [[30, 64.0, 194.0], [30, 64.0, 194.0]],\n",
    "    [[30, 646.0, 112.0], [30, 646.0, 112.0]],\n",
    "    [[40, 641.0, 213.0], [40, 641.0, 213.0]],\n",
    "    [[40, 273.0, 450.0], [40, 273.0, 450.0]],\n",
    "    [[40, 165.0, 252.0], [40, 165.0, 252.0]],\n",
    "    [[40, 44.0, 352.0], [40, 44.0, 352.0]],\n",
    "    [[40, 495.0, 310.0], [40, 495.0, 310.0]],\n",
    "    [[16, 511.0, 335.0], [16, 511.0, 335.0]],\n",
    "    [[16, 603.0, 421.0], [16, 603.0, 421.0]],\n",
    "    [[16, 78.0, 449.0], [16, 78.0, 449.0]],\n",
    "    [[16, 307.0, 326.0], [16, 307.0, 326.0]],\n",
    "    [[16, 219.0, 369.0], [16, 219.0, 369.0]],\n",
    "    [[16, 371.0, 350.0], [16, 371.0, 350.0]],\n",
    "    [[16, 448.5, 301.5], [16, 448.5, 301.5]],\n",
    "    [[16, 417.0, 224.0], [16, 417.0, 224.0]],\n",
    "    [[16, 399.0, 142.0], [16, 399.0, 142.0]],\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "SWAN = [0,1,2,20,21,22,23,24,25]\n",
    "BG=[3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "CAM=[3]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker - keep things repeatable\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "new_visibles[:,1]=1\n",
    "visibles[:,1]=1\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "new_tracks[:,:] = uncamera(new_tracks[:,:], tracks[:,CAM], T-1)\n",
    "\n",
    "new_tracks[:,BG] = new_tracks[-1:,BG]\n",
    "\n",
    "#ZOOM OUT\n",
    "mx=600\n",
    "my=250\n",
    "deltas = new_tracks+0\n",
    "deltas[:,:,0]-=mx\n",
    "deltas[:,:,1]-=my\n",
    "deltas*=.65\n",
    "deltas[:,:,0]+=mx\n",
    "deltas[:,:,1]+=my\n",
    "new_tracks=deltas\n",
    "\n",
    "#End on visible\n",
    "for bg in BG:\n",
    "    for i in reversed(range(T)):\n",
    "        v=new_visibles[i,bg]\n",
    "        x,y=tracks[i,bg]\n",
    "        if not v and x<0 or y<0 or x>=W or y>=H:\n",
    "            #MAKE OUT OF BOUNDS VISIBLE\n",
    "            new_visibles[i,bg]=1\n",
    "\n",
    "        \n",
    "# new_visibles[:,BG]=1\n",
    "\n",
    "# new_tracks += torch.randn_like(new_tracks) * 1\n",
    "\n",
    "# new_tracks[:,SWAN] = rp.blend(new_tracks[:1,SWAN], new_tracks[:,STAY], 0)\n",
    "# new_tracks[:,SWAN] = recamera(new_tracks[:,SWAN], tracks[:,CAM])\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe\n",
    "\n",
    "#### WORKER\n",
    "TITLE = \"City Biker\"\n",
    "\n",
    "input_video_path = \"CityBiker.mp4\"\n",
    "prompt = 'An aerial view of a bicyclist in a blue shirt a rides the bike viewed from above. '#It starts with the biker on the left, with three cars on the right. As the video goes on, the cars drive out of view of the camera and the biker is still in frame'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=20\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 7945\n",
    "# SEED = 9995\n",
    "# SEED = 1515\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs =[\n",
    "    [[0, 17.0, 368.0], [0, 17.0, 368.0]],\n",
    "    [[35, 124.0, 398.0], [35, 124.0, 398.0]],\n",
    "\n",
    "    [[48, 267.0, 384.0], [48, 267.0, 384.0]],\n",
    "    [[48, 270.0, 433.0], [48, 270.0, 433.0]],\n",
    "    [[29, 24.0, 433.0], [29, 24.0, 433.0]],\n",
    "\n",
    "    \n",
    "    [[11, 355.0, 206.0], [11, 355.0, 206.0]],\n",
    "    [[11, 462.0, 106.0], [11, 462.0, 106.0]],\n",
    "    [[48, 74.0, 255.0], [48, 74.0, 255.0]],\n",
    "    [[48, 265.0, 259.0], [48, 265.0, 259.0]],\n",
    "    [[48, 625.0, 325.0], [48, 625.0, 325.0]],\n",
    "    [[5, 458.0, 212.0], [5, 458.0, 212.0]],\n",
    "\n",
    "    #EXTRAS\n",
    "    [[5, 525.0, 102.0], [5, 525.0, 102.0]],\n",
    "    [[5, 313.0, 355.0], [5, 313.0, 355.0]],\n",
    "    [[5, 112.0, 118.0], [5, 112.0, 118.0]],\n",
    "    # [[5, 91.0, 236.0], [5, 91.0, 236.0]],\n",
    "    [[26, 169.5, 164.0], [26, 169.5, 164.0]],\n",
    "    [[26, 107.5, 297.0], [26, 107.5, 297.0]],\n",
    "    [[26, 657.0, 322.0], [26, 657.0, 322.0]],\n",
    "    # [[26, 627.0, 57.0], [26, 627.0, 57.0]],\n",
    "    [[26, 500.0, 68.0], [26, 500.0, 68.0]],\n",
    "    [[26, 23.0, 101.0], [26, 23.0, 101.0]],\n",
    "    [[26, 153.0, 63.0], [26, 153.0, 63.0]],\n",
    "    [[26, 422.0, 54.0], [26, 422.0, 54.0]],\n",
    "    [[26, 497.0, 218.0], [26, 497.0, 218.0]],\n",
    "    # [[14, 537.0, 265.0], [14, 537.0, 265.0]],\n",
    "    [[10, 593.0, 360.0], [10, 593.0, 360.0]],\n",
    "    [[10, 485.0, 243.0], [10, 485.0, 243.0]],\n",
    "    [[13, 446.0, 222.0], [13, 446.0, 222.0]],\n",
    "    # [[30, 561.5, 240.0], [30, 561.5, 240.0]],\n",
    "    # [[14, 454.0, 216.0], [14, 454.0, 216.0]],\n",
    "    # [[33, 237.0, 83.0], [33, 237.0, 83.0]],\n",
    "    # [[48, 105.0, 287.0], [48, 105.0, 287.0]],\n",
    "    # [[48, 343.0, 341.0], [48, 343.0, 341.0]]\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "# CAM=[3]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker - keep things repeatable\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "# new_visibles[:,1]=1\n",
    "# visibles[:,1]=1\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "\n",
    "BIKER=[0,1,2,3,4]\n",
    "\n",
    "\n",
    "new_visibles[:,BIKER]=1\n",
    "\n",
    "new_tracks[:,BIKER,0] += 100 #Shift biker to the right\n",
    "new_tracks = partial_drift(new_tracks, *BIKER, dx=100, dy=0, t_start=0, t_end=43)\n",
    "#Make them less spaced out\n",
    "# new_tracks[:33,BIKER] = (new_tracks[33:34,BIKER] - new_tracks[33:34,BIKER].mean(1,keepdim=True)) + new_tracks[:33,BIKER].mean(1,keepdim=True)\n",
    "new_tracks = partial_drift(new_tracks, 0, dx=-30, dy=0, t_start=0, t_end=43) #Parallax\n",
    "new_tracks = partial_drift(new_tracks, *BIKER, dx=-60, dy=0, t_start=28, t_end=T) #Dont go too far\n",
    "new_tracks = partial_drift(new_tracks, *BIKER, dx=0, dy=-100, t_start=0, t_end=T)\n",
    "\n",
    "new_tracks[:,BIKER,0] += 150 #Shift biker to the right\n",
    "new_tracks = partial_drift(new_tracks, *BIKER, dx=-150, dy=0, t_start=21, t_end=39)\n",
    "\n",
    "# new_tracks += torch.randn_like(new_tracks) * .125\n",
    "\n",
    "# new_tracks[:,:] = uncamera(new_tracks[:,:], tracks[:,CAM], T-1)\n",
    "\n",
    "# new_tracks[:,BG] = new_tracks[-1:,BG]\n",
    "\n",
    "# #ZOOM OUT\n",
    "# mx=600\n",
    "# my=250\n",
    "# deltas = new_tracks+0\n",
    "# deltas[:,:,0]-=mx\n",
    "# deltas[:,:,1]-=my\n",
    "# deltas*=.65\n",
    "# deltas[:,:,0]+=mx\n",
    "# deltas[:,:,1]+=my\n",
    "# new_tracks=deltas\n",
    "\n",
    "# #End on visible\n",
    "# for bg in BG:\n",
    "#     for i in reversed(range(T)):\n",
    "#         v=new_visibles[i,bg]\n",
    "#         x,y=tracks[i,bg]\n",
    "#         if not v and x<0 or y<0 or x>=W or y>=H:\n",
    "#             #MAKE OUT OF BOUNDS VISIBLE\n",
    "#             new_visibles[i,bg]=1\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe\n",
    "\n",
    "#### WORKER\n",
    "TITLE = \"[FAILURE] Spinning Ballerina\"\n",
    "\n",
    "input_video_path = \"MakeNoSpin.mp4\"\n",
    "prompt = 'A spinning ballerina '#It starts with the biker on the left, with three cars on the right. As the video goes on, the cars drive out of view of the camera and the biker is still in frame'\n",
    "\n",
    "latent_conditioning_dropout = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "guidance_scale=5\n",
    "num_inference_steps=20\n",
    "\n",
    "SEED = rp.millis() % 9999\n",
    "# SEED = 7945\n",
    "# SEED = 9995\n",
    "# SEED = 1515\n",
    "\n",
    "rp.seed_all(SEED)\n",
    "TITLE = f'[Seed {SEED}] {TITLE}'\n",
    "init_input_video()\n",
    "\n",
    "##############################\n",
    "\n",
    "#In TXY form\n",
    "\n",
    "point_pairs =[\n",
    "    [[0, 103.5, 292.0], [0, 103.5, 292.0]],\n",
    "    [[0, 453.0, 448.0], [0, 453.0, 448.0]],\n",
    "    [[0, 469.0, 207.0], [0, 469.0, 207.0]],\n",
    "    [[29, 164.0, 406.0], [29, 164.0, 406.0]],\n",
    "    [[29, 223.0, 38.0], [29, 223.0, 38.0]],\n",
    "    [[29, 354.0, 117.0], [29, 354.0, 117.0]],\n",
    "    [[29, 541.0, 247.0], [29, 541.0, 247.0]],\n",
    "    [[37, 383.0, 20.0], [37, 383.0, 20.0]],\n",
    "    [[37, 414.0, 106.0], [37, 414.0, 106.0]],\n",
    "    [[37, 448.0, 257.0], [37, 448.0, 257.0]],\n",
    "    [[44, 528.0, 41.0], [44, 528.0, 41.0]],\n",
    "    [[44, 478.0, 225.0], [44, 478.0, 225.0]],\n",
    "\n",
    "   [[44, 478.0, 225.0], [44, 478.0, 225.0]],\n",
    "    [[9, 375.0, 441.0], [9, 375.0, 441.0]],\n",
    "    [[19, 103.0, 438.0], [19, 103.0, 438.0]],\n",
    "    [[30, 106.5, 438.0], [30, 106.5, 438.0]],\n",
    "    [[41, 162.0, 240.0], [41, 162.0, 240.0]],\n",
    "    [[41, 669.0, 215.0], [41, 669.0, 215.0]],\n",
    "    [[44, 101.0, 285.0], [44, 101.0, 285.0]],\n",
    "]\n",
    "\n",
    "\n",
    "init_points = [\n",
    "    x[0] for x in point_pairs\n",
    "]\n",
    "\n",
    "CAM=[13, 1]\n",
    "\n",
    "rp.display_dict(rp.gather_vars('input_video_path prompt init_points'))\n",
    "\n",
    "init_tracks()\n",
    "\n",
    "##############################\n",
    "\n",
    "#Made without the good tracker - keep things repeatable\n",
    "visibles = cotracker_visibles\n",
    "tracks = cotracker_tracks\n",
    "\n",
    "new_tracks = tracks + 0\n",
    "new_visibles = visibles + 0\n",
    "\n",
    "\n",
    "# new_visibles[:,1]=1\n",
    "# visibles[:,1]=1\n",
    "\n",
    "def partial_drift(tracks, *i, dx=0, dy=0, t_start=0, t_end=0):\n",
    "    t_delta = t_end-t_start\n",
    "    tracks = add_drift(tracks, *i, dx=dx/t_delta, dy=dy/t_delta, t_origin=t_start, do_before=False) \n",
    "    tracks = add_drift(tracks, *i, dx=-dx/t_delta, dy=-dy/t_delta, t_origin=t_end, do_before=False) \n",
    "    return tracks\n",
    "\n",
    "\n",
    "# BIKER=[0,1,2,3,4]\n",
    "\n",
    "\n",
    "# new_visibles[:]=1\n",
    "\n",
    "# partial_drift(new_tracks, *BIKER, dx=-150, dy=0, t_start=21, t_end=39)\n",
    "\n",
    "# new_tracks += torch.randn_like(new_tracks) * .125\n",
    "\n",
    "new_tracks[:,:] = uncamera(new_tracks[:,:], tracks[:,CAM], 40)\n",
    "\n",
    "# new_tracks[:,BG] = new_tracks[-1:,BG]\n",
    "\n",
    "# #ZOOM OUT\n",
    "# mx=600\n",
    "# my=250\n",
    "# deltas = new_tracks+0\n",
    "# deltas[:,:,0]-=mx\n",
    "# deltas[:,:,1]-=my\n",
    "# deltas*=.65\n",
    "# deltas[:,:,0]+=mx\n",
    "# deltas[:,:,1]+=my\n",
    "# new_tracks=deltas\n",
    "\n",
    "# #End on visible\n",
    "# for bg in BG:\n",
    "#     for i in reversed(range(T)):\n",
    "#         v=new_visibles[i,bg]\n",
    "#         x,y=tracks[i,bg]\n",
    "#         if not v and x<0 or y<0 or x>=W or y>=H:\n",
    "#             #MAKE OUT OF BOUNDS VISIBLE\n",
    "#             new_visibles[i,bg]=1\n",
    "\n",
    "rp.display_video(gridded_video(draw_tracks(input_video, new_tracks, new_visibles)))\n",
    "display_tracks_diff()\n",
    "\n",
    "##############################\n",
    "\n",
    "init_blob_videos()\n",
    "init_sample()\n",
    "init_mp4_files()\n",
    "do_diffusion()\n",
    "save_diffusion_results()\n",
    "display_tracks_diff(output_video[:,:,:,:3])\n",
    "rp.display_video(output_video,framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
